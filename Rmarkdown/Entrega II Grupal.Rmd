---
title: "Entrega II Grupal"
description: |
  Entrega Housing Regression
author:
  - name: Irving Ram√≠rez Carrillo, Asier Zulaica Mugica, Roberto Bonilla Ibarra
    url: 
    affiliation: Universidad Complutense de Madrid
    affiliation_url: 
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        colorlinks: true
        code_folding: false
        toc: true            
        toc_depth: 3     
---

```{r setup, include = FALSE}
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 8, fig.asp = 1, out.width = "100%",
                      message = FALSE, warning = FALSE,
                      echo = TRUE, res = 400)
```

# Objetivo {#objetivo}

El objetivo de esta entrega es poder predecir el precio de vivienda usando al menos un modelo de regresi√≥n univariante (eligiendo la que mejor se considere), un modelo saturado, una regresi√≥n con selecci√≥n de modelos (BIC / AIC o penalizada) y un knn o √°rbol en modo regresi√≥n.

## Paquetes necesarios

Necesitaremos los siguientes paquetes


```{r paquetes}
# Borramos
rm(list = ls())

# Paquetes
library(skimr) # resumen num√©rico
library(tidymodels) # depuraci√≥n datos
library(tidyverse) # modelos
library(outliers) # outliers
library(timeDate) # fechas
library(ggthemes) # tema para graficar
library(ranger) # Random Forest 
library(corrr) # Crear correlaciones
library(Hmisc) # Creaci√≥n de histogramas
library(performance) # Creaci√≥n de gr√°ficas de performance para regresiones lineales.
library(corrplot) # Crear matriz de correlaci√≥n
library(parallel) # Librer√≠as de C√≥mputo en Paralelo
library(doParallel) # Librer√≠as de C√≥mputo en Paralelo
library("rpart.plot") # Librer√≠a para visualizar la l√≥gica dentro del algoritmo de √°rboles.
library(vip) # Librer√≠a para visualizar la importancia de las variables dentro del algoritmo de √°rboles.
library(olsrr) #Librer√≠a para realizar contraster de normalidad
library(car)  # Comprobar la incorrelaci√≥n de residuos
```

# Datos {#datos}

Haremos uso de un **dataset de precios de viviendas con distintas variables socioecon√≥micas.**

```{r}
viviendas_train <- read_csv(file = "../Data/house_prices_train.csv")
```

Los datos forman parte de un **registro de diferentes variables relacionadas al precio de vivienda en Ames ,Iowa** elaborado por Dean De Cock.

üìö **Detalle de variables**: <https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview>


## An√°lisis exploratorio inicial (num√©rico)

Antes de tomar ninguna decisi√≥n con los datos lo primero que deber√≠amos hacer es **echar un vistazo num√©rico** a c√≥mo se comportan las variables. Dado que vamos a clasificar, lo primero que deber√≠amos observar es como se distribuyen los niveles de nuestra variable objetivo.

### Variables

Lo primero es conocer las variables

```{r}
glimpse(viviendas_train)
```

Siendo la definici√≥n de cada variable la siguiente:

‚¶Å	SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.
‚¶Å	MSSubClass: The building class
‚¶Å	MSZoning: The general zoning classification
‚¶Å	LotFrontage: Linear feet of street connected to property
‚¶Å	LotArea: Lot size in square feet
‚¶Å	Street: Type of road access
‚¶Å	Alley: Type of alley access
‚¶Å	LotShape: General shape of property
‚¶Å	LandContour: Flatness of the property
‚¶Å	Utilities: Type of utilities available
‚¶Å	LotConfig: Lot configuration
‚¶Å	LandSlope: Slope of property
‚¶Å	Neighborhood: Physical locations within Ames city limits
‚¶Å	Condition1: Proximity to main road or railroad
‚¶Å	Condition2: Proximity to main road or railroad (if a second is present)
‚¶Å	BldgType: Type of dwelling
‚¶Å	HouseStyle: Style of dwelling
‚¶Å	OverallQual: Overall material and finish quality
‚¶Å	OverallCond: Overall condition rating
‚¶Å	YearBuilt: Original construction date
‚¶Å	YearRemodAdd: Remodel date
‚¶Å	RoofStyle: Type of roof
‚¶Å	RoofMatl: Roof material
‚¶Å	Exterior1st: Exterior covering on house
‚¶Å	Exterior2nd: Exterior covering on house (if more than one material)
‚¶Å	MasVnrType: Masonry veneer type
‚¶Å	MasVnrArea: Masonry veneer area in square feet
‚¶Å	ExterQual: Exterior material quality
‚¶Å	ExterCond: Present condition of the material on the exterior
‚¶Å	Foundation: Type of foundation
‚¶Å	BsmtQual: Height of the basement
‚¶Å	BsmtCond: General condition of the basement
‚¶Å	BsmtExposure: Walkout or garden level basement walls
‚¶Å	BsmtFinType1: Quality of basement finished area
‚¶Å	BsmtFinSF1: Type 1 finished square feet
‚¶Å	BsmtFinType2: Quality of second finished area (if present)
‚¶Å	BsmtFinSF2: Type 2 finished square feet
‚¶Å	BsmtUnfSF: Unfinished square feet of basement area
‚¶Å	TotalBsmtSF: Total square feet of basement area
‚¶Å	Heating: Type of heating
‚¶Å	HeatingQC: Heating quality and condition
‚¶Å	CentralAir: Central air conditioning
‚¶Å	Electrical: Electrical system
‚¶Å	1stFlrSF: First Floor square feet
‚¶Å	2ndFlrSF: Second floor square feet
‚¶Å	LowQualFinSF: Low quality finished square feet (all floors)
‚¶Å	GrLivArea: Above grade (ground) living area square feet
‚¶Å	BsmtFullBath: Basement full bathrooms
‚¶Å	BsmtHalfBath: Basement half bathrooms
‚¶Å	FullBath: Full bathrooms above grade
‚¶Å	HalfBath: Half baths above grade
‚¶Å	Bedroom: Number of bedrooms above basement level
‚¶Å	Kitchen: Number of kitchens
‚¶Å	KitchenQual: Kitchen quality
‚¶Å	TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)
‚¶Å	Functional: Home functionality rating
‚¶Å	Fireplaces: Number of fireplaces
‚¶Å	FireplaceQu: Fireplace quality
‚¶Å	GarageType: Garage location
‚¶Å	GarageYrBlt: Year garage was built
‚¶Å	GarageFinish: Interior finish of the garage
‚¶Å	GarageCars: Size of garage in car capacity
‚¶Å	GarageArea: Size of garage in square feet
‚¶Å	GarageQual: Garage quality
‚¶Å	GarageCond: Garage condition
‚¶Å	PavedDrive: Paved driveway
‚¶Å	WoodDeckSF: Wood deck area in square feet
‚¶Å	OpenPorchSF: Open porch area in square feet
‚¶Å	EnclosedPorch: Enclosed porch area in square feet
‚¶Å	3SsnPorch: Three season porch area in square feet
‚¶Å	ScreenPorch: Screen porch area in square feet
‚¶Å	PoolArea: Pool area in square feet
‚¶Å	PoolQC: Pool quality
‚¶Å	Fence: Fence quality
‚¶Å	MiscFeature: Miscellaneous feature not covered in other categories
‚¶Å	MiscVal: $Value of miscellaneous feature
‚¶Å	MoSold: Month Sold
‚¶Å	YrSold: Year Sold
‚¶Å	SaleType: Type of sale
‚¶Å	SaleCondition: Condition of sale

`Fuente: ` <https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data>

Adem√°s con la funci√≥n `skim()` del paquete `{skimr}` podemos **extraer algunas estad√≠sticas b√°sicas** de nuestros datos.

```{r skim}
# Resumen num√©rico
viviendas_train |> skim()
```



Podemos ver que nuestro dataset de entrenamiento cuenta con 1460 columnas, 28 columnas categ√≥ricas y 24 num√©ricas, adem√°s podemos apreciar que existen variables como `PoolQc` o `Fence` que son en su mayor√≠a nulas.


# Limpieza de datos e imputaci√≥n de nulos.

En est√° ocasi√≥n antes de proceder a realziar un an√°lisis exploratorio exhaustivo y por la necesidad de entender la correlai√≥n entre nuestras variables, procederemos a limpiar de nulos nuestros datos.


**Primero:** Uniremos los datos a√±adiendo una columna que especifique si los datos son de entrenamiento √≥ de testing.

```{r}
viviendas_test <- read_csv(file = "../Data/house_prices_test.csv")

viviendas_test <- viviendas_test |> mutate(source = "test")

viviendas_train <- viviendas_train |> mutate(source = "train")

viviendas <-  viviendas_train |> bind_rows(viviendas_test)

glimpse(viviendas)
```

**Segundo:** Analicemos los nulos de nuestro dataset completo

```{r}
# N√∫mero de valores nulos
viviendas_nulos <- viviendas |> select(-SalePrice)

null_vals <- sum(is.na(viviendas_nulos))

# C√≥lumnas con valores nulos
null_cols <- which(colSums(is.na(viviendas_nulos))>0)


sprintf(fmt="Contamos con %d valores nulos y su distribuci√≥n es la siguiente:\n", null_vals) |>  cat()

for (i in null_cols)
    {
    col_name <- names(viviendas_nulos[, i])
    col_type <- sapply(viviendas_nulos[, i],class)
    null_val <- sum(is.na(viviendas_nulos[col_name]))
    null_per <- (null_val / nrow(viviendas_nulos))*100
    sprintf(fmt = "- %s: %d (%.1f%%)  %s\n ", 
            col_name, null_val, null_per,col_type) %>% cat()
}

```

Podemos ver que 5 variable son num√©ricas y 13 son categ√≥ricas.

## Limpieza variables categ√≥ricas


### Limpieza variables Alley, FireplaceQu, GarageQual, PoolQC, BsmtQual, BsmtCond y Fence

Por la distribuci√≥n de cada una de estas variables las analizaremos y limpiaremos en la misma secci√≥n

```{r}

viviendas |>   count(Alley, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(FireplaceQu, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(GarageQual, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(PoolQC, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(BsmtQual, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(BsmtCond, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(Fence, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

```

En cada uno de ellos a√±adiremos la categoria "sin"


```{r}

viviendas <- viviendas %>% 
    mutate_at(c('PoolQC','Fence','GarageQual','FireplaceQu','Alley','BsmtQual','BsmtCond'), ~replace_na(.,"sin"))

```



### Limpieza variable MSZoning

Analicemos la distribuci√≥n de la variable MSZoning

```{r}

viviendas |>   count(MSZoning, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

```


De acuerdo a la documentaci√≥n, el significado de cada clase es el siguiente:

* A	Agriculture
* C	Commercial
* FV	Floating Village Residential
* I	Industrial
* RH	Residential High Density
* RL	Residential Low Density
* RP	Residential Low Density Park 
* RM	Residential Medium Density
       
Por lo tanto asignaremos los registros en nulo a la categor√≠a con mayor porcentaje.


```{r}

viviendas$MSZoning <- viviendas$MSZoning |> replace_na('RL')

```

### Limpieza variables Utilities, Exterior1st, Electrical, KitchenQual, Functional, SaleType

Porque los nulos se encuentran con baja densidad en estas variables, las analizaremos y limpiaremos en conjunto:

```{r}

viviendas |>   count(Utilities, sort = TRUE) |>  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(Exterior1st, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(Electrical, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(KitchenQual, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(Functional, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

viviendas |>   count(SaleType, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))

```

Podemos ver que la columna `Utilities` es pr√°cticamente de varianza cero y procederemos a removerla.

```{r}
 viviendas <- viviendas |> select(-Utilities)
```


Para las dem√°s variables hace sentido asignar la moda


```{r}
calc_mode <- function(x){
  
  # List the distinct / unique values
  distinct_values <- unique(x)
  
  # Count the occurrence of each distinct value
  distinct_tabulate <- tabulate(match(x, distinct_values))
  
  # Return the value with the highest occurrence
  distinct_values[which.max(distinct_tabulate)]
}

viviendas <- viviendas |> mutate(across(where(is.character),~replace_na(.x, calc_mode(.x))))
```


## Limpieza variables num√©ricas

Analicemos el resumen de las variables num√©ricas que tienen un valor nulo:

```{r}

null_numeric_cols <- names(which(colSums(is.na(viviendas_nulos |>  select(where(is.numeric))))>0))

viviendas |> select(null_numeric_cols) |> summary()

```

```{r}
# Decidimos quedarnos sin la variable Lot Frontage ya que LotArea hace


viviendas <- viviendas |> 
    mutate(across(c(LotFrontage),function(x) { ifelse(is.na(x), median(x, na.rm = TRUE), x) }))



viviendas <- viviendas |> 
    mutate(GarageYrBlt  = coalesce(GarageYrBlt,YearBuilt))

viviendas <- viviendas |> 
    mutate(across(c(GarageCars,GarageArea, TotalBsmtSF), ~replace_na(.x,0) ))

viviendas |> select(null_numeric_cols) |> summary()


```


# Fase 1: Muestreo

Como nuestros datos de entrenamiento son reducidos, no necesitamos hacer alguna muestra de ellos.

# Fase 2: Exploratorio

Antes de poder tomar decisiones en c√≥mo transformar nuestros datos es importante poder hacer un an√°lisis detallado visual de nuestros datos y conocer c√≥mo interact√∫an con la variable objetivo.

Para esto usaremos el tema del economist.

## Distribuci√≥n visual variable objetivo

Veamos c√≥mo se distribuye nuesta variable objetivo

```{r}
ggplot(viviendas, aes(x = SalePrice)) + 
    geom_histogram(bins = 50) +
    labs(title = "Variable Objetivo",
         subtitle = "Precio de venta",
          x = "Cantidad en d√≥lares ($)", 
         y = "Frecuencia") +
  theme_economist()

```

Podemos ver que es unimodal, con sesgo positivo y ciertos outliers que se pueden dislumbrar r√°pidamente.

## An√°lisis de variables num√©ricas


```{r}
hist.data.frame(viviendas |>   select(where(is.numeric)))
```

Podemos ver que tenemos variables num√©ricas con Outliers como la variable PoolArea, OpenPorchSF, LotArea y Lot Frontage.

Adem√°s existen variables num√©ricas que realmente representan una categor√≠a como Fireplaces, GarageCars, FirePlaces, OverallCond, MoSold, BedroomAbvGr, entre otras.

## An√°lisis de variables categ√≥ricas



```{r}

hist.data.frame(viviendas |>   select(where(is.character)))

```

Podemos ver que existen variables que necesitaremos revisar su distribuci√≥n ya que son pr√°cticamente constantes como la variable Central Air, Street, LandContour, LandSlope y PoolQC, adem√°s de que algunas variables son ordinales como ExterCond, BmstQual, BsmtCond, KitchenQual, HeatinQC, FireplaceQu, GarageQual y PoolQC. 


## Correlaci√≥n entre variables

En vez de graficar todas las relaciones entre nuestras variables, entenderemos su correlaci√≥n entre ellas y la variable objetivo, para as√≠ poder determinar qu√© an√°lisis exploratorio hace sentido.


Primero separaremos nuestro dataset train

```{r}
viviendas_cleaned_train <- viviendas |> filter(source=="train") 

```

Despu√©s realizaremos la matriz de correlaci√≥n
```{r}
cor_matrix <- viviendas_cleaned_train |> select(where(is.numeric)) |> cor() |> round(2)

cor_matrix |>
  corrplot( order = 'AOE', type = 'upper') 
```


Y veamos la variables m√°s correlacionadas:


```{r}
cor_matrix_rm <- viviendas_cleaned_train |> select(where(is.numeric)) |>select(-SalePrice) |>  cor() |> round(2)

cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
which((!apply(cor_matrix_rm, 1,function(x) any(x > 0.7)))==FALSE)

```
Podemos ver que hay 4 variables que se encuentran incorreladas

Ahora extraigamos por orden descendiente las correlaciones entre nuestras variables y `SalePrice`:

```{r}

as.data.frame(cor_matrix)  |> select(SalePrice) |> arrange(desc(SalePrice))

```

Podemos ver con claridad que las variables que quitaremos en la receta es 1stFlrSF, TotRmsAbvGrd y GarageYrBlt, ya que existen variables similares a ellas con mayor correlaci√≥n con la variable objetivo.

Con base a los resultados anteriores nos podemos preguntar lo siguiente:

* **1. ¬øQu√© relaci√≥n tiene el n√∫mero de veh√≠culos con el costo de la propiedad?**
* **2. ¬øLas casas con √°reas verdes m√°s grandes tienden a ser m√°s costosas?**
* **3. ¬øExiste una relaci√≥n directa entre el precio y la Zona?**
* **4. ¬øLas casas con mejores cocinas tienden a ser m√°s costosas?**

### ¬øQu√© relaci√≥n tiene el n√∫mero de veh√≠culos con el costo de la propiedad?

Antes de graficar podemos observar que GarageCars es un factor y por lo tanto es necesario especificarlo en ggplot.

```{r}
ggplot(viviendas_cleaned_train, aes(x = factor(GarageCars), y = SalePrice, color=factor(GarageCars))) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Pregunta 1",
       subtitle = "Distribuci√≥n del precio de venta por n√∫mero de veh√≠culos en el garage",
       x = "# Veh√≠culos", y = "Precio de Venta", color= "Veh√≠culos") +
  theme_economist()
```

Podemos ver que existe una relaci√≥n del n√∫mero de veh√≠culos con el precio de venta, sin embargo, est√° no se respeta cuando se llega al n√∫mero de 4 autos.

### ¬øLas casas con √°reas verdes m√°s grandes tienden a ser m√°s costosas?


```{r}
ggplot(viviendas_cleaned_train, aes(x =GrLivArea , y = SalePrice, size=GarageArea, color=HeatingQC)) +
  geom_point() +
  scale_color_brewer(palette="Accent")+
   labs(title = "Pregunta 2",
       subtitle = "Distribuci√≥n del precio de venta vs tama√±o del √°rea verde",
       x = "√Årea verde", y = "Precio de Venta") +
  theme_economist()
```

Podemos ver que s√≠ existe una relaci√≥n directa entre el precio de venta y el √°rea verde de la casa, adem√°s vemos que la calidad de la calefacci√≥n tambi√©n es una cualidad de las casas m√°s costosas y el amplio tama√±o del Garage.


### ¬øExiste una relaci√≥n directa entre el precio y el vecindario?


```{r}
ggplot(viviendas_cleaned_train, aes(x = MSZoning, y = SalePrice, color=MSZoning)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Pregunta 3",
       subtitle = "Distribuci√≥n del Precio de Venta y Zona",
       x = "Zona", y = "Precio de Venta", color= "Zona") +
  theme_economist()
```

Podemos ver que s√≠ existe una diferenciaci√≥n de precios por 

### ¬øLas casas con mejores cocinas tienden a ser m√°s costosas?


```{r}
ggplot(viviendas_cleaned_train, aes(x = KitchenQual, y = SalePrice, color=KitchenQual)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Pregunta 4",
       subtitle = "Distribuci√≥n del Precio de Venta y el Tipo de Cocina",
       x = "Tipo de Cocina", y = "Tipo de Cocina", color= "Tipo de Cocina") +
  theme_economist()

```
Podemos ver que s√≠ hay una relaci√≥n entre el tipo de cocina y el costo de la vivienda.

# Fase 3: Modificaci√≥n (fuera de la receta)

## Factores

Una de las primeras decisiones ser√° dotar a las variables de su **tipolog√≠a correcta**: debemos decidir si las variables de tipo texto son **variables cualitativas** (factores) o simplemente id's.

```{r}
viviendas |>
  select(where(is.character)) |>
  glimpse()
```


Todas las variables de tipo texto representan **categor√≠as de una cualitativa** as√≠ que las convertimos todas ellas a factor (modificaci√≥n estructural --> fuera de la receta)

```{r}

viviendas <- 
  viviendas |>
  mutate(across(where(is.character), as_factor))

```

Y adem√°s crearemos una nueva variable que representa el n√∫mero de ba√±os totales:
```{r}
viviendas <-
  viviendas |>
  mutate( Baths = (FullBath*2  + HalfBath))
```


Y a√±adiremos como factor las variables Baths, Fireplaces, GarageCars, Fireplaces, OverallCond, MoSold y BedroomAbvGr.

```{r}
viviendas <- 
  viviendas |>
  mutate(across(c(Baths,Fireplaces,GarageCars,Fireplaces,OverallCond,MoSold,BedroomAbvGr), as_factor))
```

Ahora veamos todos los factores que est√°n en la tabla.

```{r}
viviendas |> select(where(is.factor))
```

### Ordinales

Podemos ver que de nuestros factores, existen algunos con caracter√≠sticas ordinales.

Transformaremos los factores PoolQC, GarageQual, FireplaceQu, KitchenQual, HeatingQC, BsmtCond, BsmtQual y ExterCond

Primero veamos su distribuci√≥n:

```{r}
# Variable PoolQC
viviendas |>
  count(PoolQC) |> 
  mutate(porc = 100*n/sum(n))

# Variable GarageQual
viviendas |>
  count(GarageQual) |> 
  mutate(porc = 100*n/sum(n))

# Variable FireplaceQu
viviendas |>
  count(FireplaceQu) |> 
  mutate(porc = 100*n/sum(n))

# Variable KitchenQual
viviendas |>
  count(KitchenQual) |> 
  mutate(porc = 100*n/sum(n))

# Variable HeatingQC
viviendas |>
  count(HeatingQC) |> 
  mutate(porc = 100*n/sum(n))

# Variable BsmtCond
viviendas |>
  count(BsmtCond) |> 
  mutate(porc = 100*n/sum(n))

# Variable BsmtQual
viviendas |>
  count(BsmtQual) |> 
  mutate(porc = 100*n/sum(n))

# Variable ExterCond
viviendas |>
  count(ExterCond) |> 
  mutate(porc = 100*n/sum(n))
```


Ahora les asignaremos un orden a cada categor√≠a con base a lo que nos espec√≠fica la documentaci√≥n:

* `Ex	Excellent`
* `Gd	Good`
* `TA	Average/Typical`
* `Fa	Fair`
* `Po	Poor`

```{r}

viviendas <-
  viviendas |>
  mutate(PoolQC = factor(PoolQC, levels = c("sin", "Fa", "Gd", "Ex"),ordered = TRUE),
         GarageQual = factor(GarageQual, levels = c("sin","Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
         FireplaceQu = factor(FireplaceQu, levels = c("sin","Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
         KitchenQual = factor(KitchenQual, levels = c("Fa","TA", "Gd", "Ex"),ordered = TRUE),
         HeatingQC = factor(HeatingQC, levels = c("Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
         BsmtCond = factor(BsmtCond, levels = c("sin","Po", "Fa","TA", "Gd"),ordered = TRUE),
         BsmtQual = factor(BsmtCond, levels = c("sin", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
         ExterCond = factor(ExterCond, levels = c("Fa","Po","TA", "Gd", "Ex"),ordered = TRUE)
         )
knitr::kable(viviendas |> select(PoolQC, GarageQual, FireplaceQu, KitchenQual, HeatingQC, BsmtCond, BsmtQual, ExterCond) |> slice_sample(n=10) )
```

## Separaci√≥n Original

Por √∫ltimo separaremos nuestro dataset en Train y Test como originalmente se encontraba.


```{r}

viviendas_train <- viviendas |> filter(source=="train") |> select(-source)


viviendas_test <- viviendas |> filter(source=="test") |> select(-source)

```

# Fase 3: Modificaci√≥n (dentro de la receta)


### Partici√≥n

#### Train - Test Partici√≥n

Partiremos nuestros datos en train 85% y test 15%, esto lo usaremos para encontrar los mejores par√°metros.

```{r}
# Partici√≥n 10% de test 
viviendas_split <- initial_split(viviendas_train, prop = 0.85)
viviendas_split
```

F√≠jate que en `viviendas_split` solo tenemos las instrucciones. Vamos a aplicarlas

```{r}
# Aplicamos partici√≥n
train_data <- training(viviendas_split)
test_data <- testing(viviendas_split)
```


#### Validaci√≥n por Partici√≥n 

Est√° validaci√≥n la usaremos para los modelos univariantes, modelos saturados y regresiones con selecci√≥n de modelos.
```{r}
# Validaci√≥n
validation_data <- validation_split(viviendas_train, prop = 0.7)
```


#### Validaci√≥n Cruzada

Para los modelos que necesitaremos hiperparametrizar usaremos validaci√≥n cruzada con repetici√≥n.

```{r}
# Fijamos la semilla
set.seed(100)

# Declaramos el n√∫mero de particiones en las que procederemos a validar.
cv_folds <-
 vfold_cv(train_data, 
          v = 4, 
          repeats = 1) 
```


### Roles

Tras las particiones, el primer paso es **definir la receta**, indic√°ndole el conjunto donde tenemos validaci√≥n y train. Despu√©s lo que haremos ser√° **asignar posibles roles** que nos puedan diferencias las acciones entre las variables


```{r}
# Receta
rec_viviendas <-
  # F√≥rmula y datos
  recipe(data = train_data, SalePrice ~ .)|>
  # Roles
  add_role(where(is.factor), new_role = "cuali") |> 
  add_role(where(is.numeric), new_role = "cuanti") |> 
  add_role(c(Id, FullBath, HalfBath, '1stFlrSF', TotRmsAbvGrd,GarageYrBlt ), new_role = "Drop_Columns")
```


###Receta Regresi√≥n Univariante

Ahora procederemos en crear la receta para la regresi√≥n univariante.

```{r}
# Receta
rec_reg_viviendas <-
  # F√≥rmula y datos
  recipe(data = train_data, SalePrice ~ GrLivArea) %>% 
  step_log(GrLivArea, base = 10) %>%
  # los dem√°s imputamos por la media
  step_mutate(GrLivArea = ifelse(abs(scores(GrLivArea, type = "z")) > 2.5, NA, GrLivArea)) %>% 
  step_impute_mean(GrLivArea)
rec_reg_viviendas
```

Probaremos la receta para ver sus resultados:

```{r}
prepped_data <- 
  rec_reg_viviendas |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```

### Receta Modelo Saturado

Ahora procederemos en crear la receta para la regresi√≥n saturada.

```{r}
# Receta
rec_reg_saturada <-
  # F√≥rmula y datos
  rec_viviendas |> 
  step_rm(has_role("Drop_Columns")) |> 
  step_mutate(across(all_numeric_predictors(), function(x) { ifelse(abs(scores(x,type = "z")) > 2.5 & !is.na(x), NA, x) })) |> 
  step_impute_bag(all_numeric_predictors()) |> 
  step_impute_mode(has_role("cuali")) |>
  step_BoxCox(all_numeric_predictors()) |>
  step_other(has_role("cuali"), threshold = .1, other = "Otros")  |> 
  # Normalizamos
  step_dummy(all_nominal(), -all_outcomes())  |> 
  step_normalize(all_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.8)
rec_reg_saturada
```

Probaremos la receta para ver sus resultados:

```{r}
prepped_data <- 
  rec_reg_saturada |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```


### Receta Regresi√≥n Penalizada (Elastic Net)

Ahora procederemos en crear la receta para la regresi√≥n penalizada usando el modelo Elastic Net.

```{r}
# Receta
rec_reg_elastic <-
  # F√≥rmula y datos
  rec_viviendas |> 
  step_rm(has_role("Drop_Columns")) |> 
  step_mutate(across(all_numeric_predictors(), function(x) { ifelse(abs(scores(x,type = "z")) > 2.2 & !is.na(x), NA, x) })) |> 
  step_impute_knn(all_numeric_predictors()) |> 
  step_impute_mode(has_role("cuali")) |>
  step_BoxCox(all_numeric_predictors()) |>
  step_sqrt(SalePrice) |> 
  step_other(has_role("cuali"), threshold = .2, other = "Otros")  |> 
  # Normalizamos
  step_dummy(all_nominal(), -all_outcomes())  |> 
  step_normalize(all_predictors()) |> 
  step_zv(all_predictors()) |> 
  step_corr(all_numeric_predictors(), threshold = 0.8)
rec_reg_elastic
```

Probaremos la receta para ver sus resultados:

```{r}
prepped_data <- 
  rec_reg_elastic |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```



# Fase 4 Modelling

## Fase 4 Regresi√≥n Univariante

### Flujo y Evaluaci√≥n Regresi√≥n Univariante


Primero generamos el modelo lineal, el flujo y evaluamos el modelo con los datos de validaci√≥n.

```{r}
# Modelo lineal
reg_lineal <- linear_reg() %>% set_mode("regression") %>% set_engine("lm")

# Flujo de Trabajo
reg_wflow_viviendas <-
  workflow() %>% 
  add_model(reg_lineal) %>% 
  add_recipe(rec_reg_viviendas)

# Evaluaci√≥n del modelo a los datos de validaci√≥n
reg_fit_viviendas <- 
  reg_wflow_viviendas %>%
  fit_resamples(resamples = validation_data)

# M√©tricas de error
reg_fit_viviendas |> collect_metrics()


```
Podemos ver que el R cuadrada es de .45 y el RMSE es de 55,906 d√≥lares, esto nos da a simple vista resultados de un modelo muy pobre en rendimiento.


### Interpretaci√≥n de resultados

```{r}
reg_fit_viviendas <-  reg_wflow_viviendas %>% fit(viviendas_train)
tidy(reg_fit_viviendas)
```

La predicci√≥n del precio de la vivienda es -1,012,224 d√≥lares cuando el logaritmo de la variable GrLivArea es 0, y por cada unidad en que el logaritmo base 10 aumenta el precio de la vivienda aumenta 378115.2 d√≥lares

### Representaci√≥n de resultados

Hagamos la representaci√≥n de los resultados:

```{r}
check_model(reg_fit_viviendas %>% extract_fit_engine())
```

#### Intervalos de confianza de la estimaci√≥n

Podemos ver los coeficientes en los que ronda el 95% de probabilidad en que los par√°metros sean los adecuados, mas no la poblaci√≥n.


```{r}
confint(reg_fit_viviendas %>% extract_fit_engine())
```

Para esto es importante citar la fuente <https://www.investopedia.com/terms/c/confidenceinterval.asp>

**What Is a Common Misconception About Confidence Intervals?**

The biggest misconception regarding confidence intervals is that they represent the percentage of data from a given sample that falls between the upper and lower bounds. In other words, it would be incorrect to assume that a 99% confidence interval means that 99% of the data in a random sample falls between these bounds. What it actually means is that one can be 99% certain that the range will contain the population mean.

```{r echo = FALSE, results = 'asis'}
image = "https://www.investopedia.com/thmb/mgpezimLowCvuivu5aBE_dChWDI=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/ConfidenceInterval-387c2dddb10c457e9d6041039b5b6e2c.png"
cat(paste0('<center><img src="', image,  '"></center>')) 
```


#### Linealidad

Se espera que entre los residuos no exista tendencia significativa, sin embargo, podemos ver que nuestros residuos siguen una tendencia en forma de U, esto puede ser porque le falta informaci√≥n al modelo.

#### ANOVA (Analysis of Variance)

Haciendo un test anova de nuestros predictores vs los residuos, tenemos lo siguientes resultados con respecto a una relaci√≥n lineal:

```{r}
ajuste <- reg_fit_viviendas %>% extract_fit_engine()
lm(ajuste$residuals ~ ajuste$fitted.values) %>% anova()
```


Vemos que el p-valor Pr(>F) de la prueba realizada es bastante grande, por lo que no parece existir tendencia lineal entre los residuos.


Ahora lo haremos revisando la tendencia cuadr√°tica.
```{r}
lm(ajuste$residuals ~ I(ajuste$fitted.values^2) + ajuste$fitted.values) %>% anova()
```

Podemos ver que sigue sin existir una tendencia cuadr√°tica


#### Homecedasticidad

```{r}
#Revisar Homecedasticidad
check_heteroscedasticity(ajuste)
```


Podemos ver que no existe `Homecedasticidad` en los residuos, por lo tanto deber√≠amos probar a transformar la predictora a ra√≠z cuadrada o logaritmo.

#### Diagnosis de Regresi√≥n 

Grafiquemos los residuales

```{r}
ggplot(
  tibble("obs" = 1:length(ajuste$residuals),
         "res" = ajuste$residuals),
  aes(x = obs, y = res)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Diagnosis de Regresi√≥n", subtitle="Modelo Univariante")+
  theme_economist()
```
Podemos ver que no existe una banda de error definida.


#### Normalidad de los residuos

Podemos ver que en ninguno de los casos supera el test de normalidad, e incluso la recta en el gr√°fico QQ Plot no se sigue, por lo tanto, la recomendaci√≥n es transformar la objetivo tomando ra√≠z cuadrada, logaritmo o transformaciones Box-Cox para variables positivas, o transformaciones Yeo-Johnson para variables que puedan tomar valores negativos.

```{r}
ols_test_normality(ajuste$residuals)
```

#### Incorrelaci√≥n de residuos

```{r}

durbinWatsonTest(ajuste)
```
No hay evidencia suficiente para rechazar la hip√≥tesis nula, por lo tanto no podemos asegurar que se encuentran incorrelados.

#### Resumen de evaluaci√≥n del modelo

```{r}
glance(reg_fit_viviendas)
```

#### Predicci√≥n

Ahora realizaremos la evaluaci√≥n y predicci√≥n en Test.

```{r}
# Predecimos en tst
fit_viviendas <- reg_wflow_viviendas %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_viviendas %>% collect_metrics()
```

Vemos que las m√©tricas son muy similares al set de validaci√≥n.

```{r}
# Errores en test
pred_test <-
  fit_viviendas %>%
  collect_predictions() %>%
  mutate(error = SalePrice - .pred)
pred_test
```

Calculamos el error y graficamos la predicci√≥n con el valor real.


```{r}
# Gr√°ficos en test
ggplot(data = pred_test,
       mapping = aes(x = .pred,
                     y = SalePrice)) +
  geom_point(color = "#006EA1",
             alpha = 0.6,
             size = 4) +
  # Diagonal
  geom_abline(intercept = 0, slope = 1,
              color = "orange", size = 1.2) +
  theme_economist() + 
  labs(title = "Resultados regresi√≥n lineal univariante",
       subtitle =
         "Valores deber√≠an estar cercanos a la diagonal",
       caption =
         "Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
       x = "Predicciones",
       y = "Valores reales")
```

Podemos ver que los valores est√°n cercanos a la diagonal cuando las casas son de precio medio y bajo, sin embargo, con las casas de precio alto la predicci√≥n tiene un error muy alto.


## Fase 4 Regresi√≥n Saturada

### Flujo y Evaluaci√≥n Regresi√≥n Saturada


Primero generamos el modelo lineal, el flujo y evaluamos el modelo con los datos de validaci√≥n.

```{r}
# Modelo lineal
reg_lineal <- linear_reg() %>% set_mode("regression") %>% set_engine("lm")

# Flujo de Trabajo
reg_wflow_saturada <-
  workflow() %>% 
  add_model(reg_lineal) %>% 
  add_recipe(rec_reg_saturada)

# Evaluaci√≥n del modelo a los datos de validaci√≥n
reg_fit_saturada <- 
  reg_wflow_saturada %>%
  fit_resamples(resamples = validation_data)

# M√©tricas de error
reg_fit_saturada |> collect_metrics()


```
Podemos ver que el R cuadrada es de .8 y el RMSE es de 3,647 d√≥lares, esto nos da mejores resultados que la regresi√≥n univariante.

### Interpretaci√≥n de resultados

```{r}
reg_fit_saturada <-  reg_wflow_saturada %>% fit(viviendas_train)
tidy(reg_fit_saturada)
```

La predicci√≥n del precio de la vivienda es -1,012,224 d√≥lares cuando el eje 'x' se posiciona en 0 y podemos ver los coeficientes que toman cada una de nuestras variables.

### Representaci√≥n de resultados

Hagamos la representaci√≥n de los resultados:

```{r}
check_model(reg_fit_saturada %>% extract_fit_engine())
```
Podemos ver que existe colinealidad en nuestras variables y eso puede afectar con ruido en la regresi√≥n.

#### Intervalos de confianza de la estimaci√≥n

Podemos ver los coeficientes en los que ronda el 95% de probabilidad en que los par√°metros sean los adecuados, mas no la poblaci√≥n.


```{r}
confint(reg_fit_saturada %>% extract_fit_engine())
```


#### Linealidad

Se espera que entre los residuos no exista tendencia significativa, y en el gr√°fico podemos observar que el patr√≥n que se sigue es el esperado donde los residuos no se alejan con alguna tendencia de la predicci√≥n.

#### ANOVA (Analysis of Variance)

Haciendo un test anova de nuestros predictores vs los residuos, tenemos lo siguientes resultados con respecto a una relaci√≥n lineal:

```{r}
ajuste <- reg_fit_saturada %>% extract_fit_engine()
lm(ajuste$residuals ~ ajuste$fitted.values) %>% anova()
```


Vemos que el p-valor Pr(>F) de la prueba realizada es bastante grande, por lo que no parece existir tendencia lineal entre los residuos.


Ahora lo haremos revisando la tendencia cuadr√°tica.
```{r}
lm(ajuste$residuals ~ I(ajuste$fitted.values^2) + ajuste$fitted.values) %>% anova()
```

Podemos ver que sigue sin existir una tendencia cuadr√°tica


#### Homecedasticidad

```{r}
#Revisar Homecedasticidad
check_heteroscedasticity(ajuste)
```


Podemos ver que no existe `Homecedasticidad` en los residuos, por lo tanto deber√≠amos probar a transformar la predictora a ra√≠z cuadrada o logaritmo.

#### Diagnosis de Regresi√≥n 

Grafiquemos los residuales

```{r}
ggplot(
  tibble("obs" = 1:length(ajuste$residuals),
         "res" = ajuste$residuals),
  aes(x = obs, y = res)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Diagnosis de Regresi√≥n", subtitle="Modelo Univariante")+
  theme_economist()
```
Podemos ver que no existe una banda de error definida, sin embargo los residuos parecen seguir una recta en su tendencia (lo cual es adecuado).

#### Normalidad de los residuos

Podemos ver que en ninguno de los casos supera el test de normalidad, e incluso la recta en el gr√°fico QQ Plot no se sigue, por lo tanto, la recomendaci√≥n es transformar la objetivo tomando ra√≠z cuadrada, logaritmo o transformaciones Box-Cox para variables positivas, o transformaciones Yeo-Johnson para variables que puedan tomar valores negativos.

```{r}
ols_test_normality(ajuste$residuals)
```

#### Incorrelaci√≥n de residuos

```{r}

durbinWatsonTest(ajuste)
```
No hay evidencia suficiente para rechazar la hip√≥tesis nula, por lo tanto no podemos asegurar que se encuentran incorrelados.

#### Resumen de evaluaci√≥n del modelo

```{r}
glance(reg_fit_saturada)
```

#### Predicci√≥n

Ahora realizaremos la evaluaci√≥n y predicci√≥n en Test.

```{r}
# Predecimos en tst
fit_saturada <- reg_wflow_saturada %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_saturada %>% collect_metrics()
```

Vemos que las m√©tricas son muy similares al set de validaci√≥n, e incluso tenemos una reducci√≥n en el RMSE.

```{r}
# Errores en test
pred_test <-
  fit_saturada %>%
  collect_predictions() %>%
  mutate(error = SalePrice - .pred)
pred_test
```

Calculamos el error y graficamos la predicci√≥n con el valor real.


```{r}
# Gr√°ficos en test
ggplot(data = pred_test,
       mapping = aes(x = .pred,
                     y = SalePrice)) +
  geom_point(color = "#006EA1",
             alpha = 0.6,
             size = 4) +
  # Diagonal
  geom_abline(intercept = 0, slope = 1,
              color = "orange", size = 1.2) +
  theme_economist() + 
  labs(title = "Resultados regresi√≥n lineal saturada",
       subtitle =
         "Valores deber√≠an estar cercanos a la diagonal",
       caption =
         "Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
       x = "Predicciones",
       y = "Valores reales")
```

Podemos ver que los valores est√°n cercanos a la diagonal con mucha m√°s frecuencia, sin embargo, los residuales siguen un poco separados de la recta.



## Fase 4 Regresi√≥n Penalizada (Elastic Net)

### Definici√≥n del Modelo


Primero generamos el modelo definido por el motor glmnet donde mixture ser√° el par√°metro Œ± y penalty el par√°metro Œª, el flujo y evaluamos el modelo con los datos de validaci√≥n.

```{r}
# Modelo lineal
elastic_net <-
  linear_reg(mixture = tune("alpha"), penalty = tune("lambda")) %>%
  set_mode("regression") %>% set_engine("glmnet")
```


### Grid de par√°metros

Y vamos a generar un grid de par√°metros (200 regresiones) usando penalty() (f√≠jate que est√° en escala logar√≠tmica, es decir, que -2 es una penalizaci√≥n de 0.01 y 2 de 100)

```{r}
grid_elastic_net <-
  expand_grid("alpha" = seq(0, 1, l = 5),
              "lambda" = grid_regular(penalty(range = c(-4, 1)), levels = 40) %>%
                pull(penalty))
```


### Hiperparametrizaci√≥n

```{r}
wflow_elastic <-
  workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)

mod_elastic <- wflow_elastic %>%
  tune_grid(resamples = cv_folds, grid = grid_elastic_net,
            control = control_grid(verbose = TRUE))

```

### Selecci√≥n del mejor modelo

```{r}
# Muestra del mejor modelo en R cuadrada
mod_elastic %>% show_best("rsq")

# Muestra del mejor modelo en RMSE
mod_elastic %>% show_best("rmse")

# Selecci√≥n del mejor modelo en R cuadrada
mod_elastic %>% select_best("rsq")

# M√©tricas de error
mod_elastic |> collect_metrics()

```
Podemos ver que el R cuadrada es de .8 y el RMSE es de 37.04, que en valor real de la objetivo es el cuadrado de este n√∫mero siendo 1371 d√≥lares esto nos da mejores resultados que la regresi√≥n multivariante y saturada.


### Gr√°fica de las m√©tricas en funci√≥n de los par√°metros

```{r}
mod_elastic %>% autoplot()
```
Podemos ver que los mejores resultados se pueden percibir cuando el valor lambda es alto y alpha tiene un valor de .25

### Construcci√≥n Modelo Final

```{r}
elastic_net <-
  linear_reg(mixture = 0.25, penalty =7.443803) %>%
  set_mode("regression") %>% set_engine("glmnet")
wflow_elastic <-
  workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)
mod_elastic_fit <-
  wflow_elastic %>% fit(train_data)
```

### Coeficientes Modelo FInal

Hagamos la representaci√≥n del modelo resultante.

```{r}
tidy(mod_elastic_fit)
```

Veamos las variables que ha dejado fuera el modelo:


```{r}
coeficientes <- tidy(mod_elastic_fit)
coeficientes |> filter(estimate==0)
```
Podemos ver las 24 variables que se han quedado fuera de acuerdo al modelo el√°stico.

### Predicci√≥n

Ahora realizaremos la evaluaci√≥n y predicci√≥n en Test.

```{r}
# Predecimos en tst
fit_elastic <- mod_elastic_fit %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_elastic%>% collect_metrics()
```

Vemos que las m√©tricas son muy similares al set de validaci√≥n, e incluso tenemos una reducci√≥n en el RMSE.

```{r}
# Errores en test
pred_test <-
  fit_elastic%>%
  collect_predictions() %>%
  mutate(error = SalePrice - .pred)
pred_test
```

Calculamos el error y graficamos la predicci√≥n con el valor real.


```{r}
# Gr√°ficos en test
ggplot(data = pred_test,
       mapping = aes(x = .pred,
                     y = SalePrice)) +
  geom_point(color = "#006EA1",
             alpha = 0.6,
             size = 4) +
  # Diagonal
  geom_abline(intercept = 0, slope = 1,
              color = "orange", size = 1.2) +
  theme_economist() + 
  labs(title = "Resultados regresi√≥n lineal saturada",
       subtitle =
         "Valores deber√≠an estar cercanos a la diagonal",
       caption =
         "Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
       x = "Predicciones",
       y = "Valores reales")
```

Podemos ver que los valores est√°n cercanos a la diagonal y el error est√° con varianza controlada.


### Diagnosis de Regresi√≥n 

Grafiquemos los residuales

```{r}
ggplot(
  tibble("obs" = 1:length(pred_test$error),
         "res" = pred_test$error),
  aes(x = obs, y = res)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(title="Diagnosis de Regresi√≥n", subtitle="Modelo Univariante")+
  theme_economist()
```
Podemos ver que no existe una banda de error definida, sin embargo los residuos parecen seguir una recta en su tendencia (lo cual es adecuado).

#### Normalidad de los residuos

```{r}
ols_test_normality(pred_test$error)
```

La normalidad en los residuos se cumple en el test de Kolmogorov-Smirnov.

