aes(x = obs, y = res)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title="Diagnosis de Regresión", subtitle="Modelo Univariante")+
theme_economist()
ols_test_normality(ajuste$residuals)
durbinWatsonTest(ajuste)
glance(reg_fit_saturada)
# Predecimos en tst
fit_saturada <- reg_wflow_saturada %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_saturada %>% collect_metrics()
# Errores en test
pred_test <-
fit_saturada %>%
collect_predictions() %>%
mutate(error = SalePrice - .pred)
pred_test
# Gráficos en test
ggplot(data = pred_test,
mapping = aes(x = .pred,
y = SalePrice)) +
geom_point(color = "#006EA1",
alpha = 0.6,
size = 4) +
# Diagonal
geom_abline(intercept = 0, slope = 1,
color = "orange", size = 1.2) +
theme_economist() +
labs(title = "Resultados regresión lineal saturada",
subtitle =
"Valores deberían estar cercanos a la diagonal",
caption =
"Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
x = "Predicciones",
y = "Valores reales")
# Modelo lineal
elastic_net <-
linear_reg(mixture = tune("alpha"), penalty = tune("lambda")) %>%
set_mode("regression") %>% set_engine("glmnet")
grid_elastic_net <-
expand_grid("alpha" = seq(0, 1, l = 5),
"lambda" = grid_regular(penalty(range = c(-4, 1)), levels = 40) %>%
pull(penalty))
wflow_elastic <-
workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)
mod_elastic <- wflow_elastic %>%
tune_grid(resamples = cv_folds, grid = grid_elastic_net,
control = control_grid(verbose = TRUE))
# Selección del mejor modelo
mod_elastic %>% show_best("rsq")
# Selección del mejor modelo
mod_elastic %>% show_best("rmse")
# Selección del mejor modelo
mod_elastic %>% select_best("rsq")
# Métricas de error
mod_elastic |> collect_metrics()
mod_elastic %>% autoplot()
elastic_net <-
linear_reg(mixture = 0.25, penalty =7.443803) %>%
set_mode("regression") %>% set_engine("glmnet")
wflow_elastic <-
workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)
mod_elastic_fit <-
wflow_elastic %>% fit(train_data)
tidy(mod_elastic)
tidy(mod_elastic_fit)
# Predecimos en tst
fit_elastic <- mod_elastic_fit %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_elastic%>% collect_metrics()
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 8, fig.asp = 1, out.width = "100%",
message = FALSE, warning = FALSE,
echo = TRUE, res = 400)
# Borramos
rm(list = ls())
# Paquetes
library(skimr) # resumen numérico
library(tidymodels) # depuración datos
library(tidyverse) # modelos
library(outliers) # outliers
library(timeDate) # fechas
library(ggthemes) # tema para graficar
library(ranger) # Random Forest
library(corrr) # Crear correlaciones
library(Hmisc) # Creación de histogramas
library(performance) # Creación de gráficas de performance para regresiones lineales.
library(corrplot) # Crear matriz de correlación
library(parallel) # Librerías de Cómputo en Paralelo
library(doParallel) # Librerías de Cómputo en Paralelo
library("rpart.plot") # Librería para visualizar la lógica dentro del algoritmo de árboles.
library(vip) # Librería para visualizar la importancia de las variables dentro del algoritmo de árboles.
library(olsrr) #Librería para realizar contraster de normalidad
library(car)  # Comprobar la incorrelación de residuos
viviendas_train <- read_csv(file = "../Data/house_prices_train.csv")
glimpse(viviendas_train)
# Resumen numérico
viviendas_train |> skim()
viviendas_test <- read_csv(file = "../Data/house_prices_test.csv")
viviendas_test <- viviendas_test |> mutate(source = "test")
viviendas_train <- viviendas_train |> mutate(source = "train")
viviendas <-  viviendas_train |> bind_rows(viviendas_test)
glimpse(viviendas)
# Número de valores nulos
viviendas_nulos <- viviendas |> select(-SalePrice)
null_vals <- sum(is.na(viviendas_nulos))
# Cólumnas con valores nulos
null_cols <- which(colSums(is.na(viviendas_nulos))>0)
sprintf(fmt="Contamos con %d valores nulos y su distribución es la siguiente:\n", null_vals) |>  cat()
for (i in null_cols)
{
col_name <- names(viviendas_nulos[, i])
col_type <- sapply(viviendas_nulos[, i],class)
null_val <- sum(is.na(viviendas_nulos[col_name]))
null_per <- (null_val / nrow(viviendas_nulos))*100
sprintf(fmt = "- %s: %d (%.1f%%)  %s\n ",
col_name, null_val, null_per,col_type) %>% cat()
}
viviendas |>   count(Alley, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(FireplaceQu, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(GarageQual, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(PoolQC, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(BsmtQual, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(BsmtCond, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(Fence, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas <- viviendas %>%
mutate_at(c('PoolQC','Fence','GarageQual','FireplaceQu','Alley','BsmtQual','BsmtCond'), ~replace_na(.,"sin"))
viviendas |>   count(MSZoning, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas$MSZoning <- viviendas$MSZoning |> replace_na('RL')
viviendas |>   count(Utilities, sort = TRUE) |>  mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(Exterior1st, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(Electrical, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(KitchenQual, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(Functional, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas |>   count(SaleType, sort = TRUE) |>
mutate(porc = 100*n/sum(n), cumsum(porc))
viviendas <- viviendas |> select(-Utilities)
calc_mode <- function(x){
# List the distinct / unique values
distinct_values <- unique(x)
# Count the occurrence of each distinct value
distinct_tabulate <- tabulate(match(x, distinct_values))
# Return the value with the highest occurrence
distinct_values[which.max(distinct_tabulate)]
}
viviendas <- viviendas |> mutate(across(where(is.character),~replace_na(.x, calc_mode(.x))))
null_numeric_cols <- names(which(colSums(is.na(viviendas_nulos |>  select(where(is.numeric))))>0))
viviendas |> select(null_numeric_cols) |> summary()
# Decidimos quedarnos sin la variable Lot Frontage ya que LotArea hace
viviendas <- viviendas |>
mutate(across(c(LotFrontage),function(x) { ifelse(is.na(x), median(x, na.rm = TRUE), x) }))
viviendas <- viviendas |>
mutate(GarageYrBlt  = coalesce(GarageYrBlt,YearBuilt))
viviendas <- viviendas |>
mutate(across(c(GarageCars,GarageArea, TotalBsmtSF), ~replace_na(.x,0) ))
viviendas |> select(null_numeric_cols) |> summary()
ggplot(viviendas, aes(x = SalePrice)) +
geom_histogram(bins = 50) +
labs(title = "Variable Objetivo",
subtitle = "Precio de venta",
x = "Cantidad en dólares ($)",
y = "Frecuencia") +
theme_economist()
hist.data.frame(viviendas |>   select(where(is.numeric)))
hist.data.frame(viviendas |>   select(where(is.character)))
viviendas_cleaned_train <- viviendas |> filter(source=="train")
cor_matrix <- viviendas_cleaned_train |> select(where(is.numeric)) |> cor() |> round(2)
cor_matrix |>
corrplot( order = 'AOE', type = 'upper')
cor_matrix_rm <- viviendas_cleaned_train |> select(where(is.numeric)) |>select(-SalePrice) |>  cor() |> round(2)
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
which((!apply(cor_matrix_rm, 1,function(x) any(x > 0.7)))==FALSE)
as.data.frame(cor_matrix)  |> select(SalePrice) |> arrange(desc(SalePrice))
ggplot(viviendas_cleaned_train, aes(x = factor(GarageCars), y = SalePrice, color=factor(GarageCars))) +
geom_boxplot() +
stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
labs(title = "Pregunta 1",
subtitle = "Distribución del precio de venta por número de vehículos en el garage",
x = "# Vehículos", y = "Precio de Venta", color= "Vehículos") +
theme_economist()
ggplot(viviendas_cleaned_train, aes(x =GrLivArea , y = SalePrice, size=GarageArea, color=HeatingQC)) +
geom_point() +
scale_color_brewer(palette="Accent")+
labs(title = "Pregunta 2",
subtitle = "Distribución del precio de venta vs tamaño del área verde",
x = "Área verde", y = "Precio de Venta") +
theme_economist()
ggplot(viviendas_cleaned_train, aes(x = MSZoning, y = SalePrice, color=MSZoning)) +
geom_boxplot() +
stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
labs(title = "Pregunta 3",
subtitle = "Distribución del Precio de Venta y Zona",
x = "Zona", y = "Precio de Venta", color= "Zona") +
theme_economist()
ggplot(viviendas_cleaned_train, aes(x = KitchenQual, y = SalePrice, color=KitchenQual)) +
geom_boxplot() +
stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
labs(title = "Pregunta 4",
subtitle = "Distribución del Precio de Venta y el Tipo de Cocina",
x = "Tipo de Cocina", y = "Tipo de Cocina", color= "Tipo de Cocina") +
theme_economist()
viviendas |>
select(where(is.character)) |>
glimpse()
viviendas <-
viviendas |>
mutate(across(where(is.character), as_factor))
viviendas <-
viviendas |>
mutate( Baths = (FullBath*2  + HalfBath))
viviendas <-
viviendas |>
mutate(across(c(Baths,Fireplaces,GarageCars,Fireplaces,OverallCond,MoSold,BedroomAbvGr), as_factor))
viviendas |> select(where(is.factor))
# Variable PoolQC
viviendas |>
count(PoolQC) |>
mutate(porc = 100*n/sum(n))
# Variable GarageQual
viviendas |>
count(GarageQual) |>
mutate(porc = 100*n/sum(n))
# Variable FireplaceQu
viviendas |>
count(FireplaceQu) |>
mutate(porc = 100*n/sum(n))
# Variable KitchenQual
viviendas |>
count(KitchenQual) |>
mutate(porc = 100*n/sum(n))
# Variable HeatingQC
viviendas |>
count(HeatingQC) |>
mutate(porc = 100*n/sum(n))
# Variable BsmtCond
viviendas |>
count(BsmtCond) |>
mutate(porc = 100*n/sum(n))
# Variable BsmtQual
viviendas |>
count(BsmtQual) |>
mutate(porc = 100*n/sum(n))
# Variable ExterCond
viviendas |>
count(ExterCond) |>
mutate(porc = 100*n/sum(n))
viviendas <-
viviendas |>
mutate(PoolQC = factor(PoolQC, levels = c("sin", "Fa", "Gd", "Ex"),ordered = TRUE),
GarageQual = factor(GarageQual, levels = c("sin","Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
FireplaceQu = factor(FireplaceQu, levels = c("sin","Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
KitchenQual = factor(KitchenQual, levels = c("Fa","TA", "Gd", "Ex"),ordered = TRUE),
HeatingQC = factor(HeatingQC, levels = c("Po", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
BsmtCond = factor(BsmtCond, levels = c("sin","Po", "Fa","TA", "Gd"),ordered = TRUE),
BsmtQual = factor(BsmtCond, levels = c("sin", "Fa","TA", "Gd", "Ex"),ordered = TRUE),
ExterCond = factor(ExterCond, levels = c("Fa","Po","TA", "Gd", "Ex"),ordered = TRUE)
)
knitr::kable(viviendas |> select(PoolQC, GarageQual, FireplaceQu, KitchenQual, HeatingQC, BsmtCond, BsmtQual, ExterCond) |> slice_sample(n=10) )
viviendas_train <- viviendas |> filter(source=="train") |> select(-source)
viviendas_test <- viviendas |> filter(source=="test") |> select(-source)
# Partición 10% de test
viviendas_split <- initial_split(viviendas_train, prop = 0.85)
viviendas_split
# Aplicamos partición
train_data <- training(viviendas_split)
test_data <- testing(viviendas_split)
# Validación
validation_data <- validation_split(viviendas_train, prop = 0.7)
# Fijamos la semilla
set.seed(100)
# Declaramos el número de particiones en las que procederemos a validar.
cv_folds <-
vfold_cv(train_data,
v = 4,
repeats = 1)
# Receta
rec_viviendas <-
# Fórmula y datos
recipe(data = train_data, SalePrice ~ .)|>
# Roles
add_role(where(is.factor), new_role = "cuali") |>
add_role(where(is.numeric), new_role = "cuanti") |>
add_role(c(Id, FullBath, HalfBath, '1stFlrSF', TotRmsAbvGrd,GarageYrBlt ), new_role = "Drop_Columns")
# Receta
rec_reg_viviendas <-
# Fórmula y datos
recipe(data = train_data, SalePrice ~ GrLivArea) %>%
step_log(GrLivArea, base = 10) %>%
# los demás imputamos por la media
step_mutate(GrLivArea = ifelse(abs(scores(GrLivArea, type = "z")) > 2.5, NA, GrLivArea)) %>%
step_impute_mean(GrLivArea)
rec_reg_viviendas
prepped_data <-
rec_reg_viviendas |>  # use the recipe object
prep() |>  # perform the recipe on training data
juice() # extract only the preprocessed dataframe
glimpse(prepped_data)
# Receta
rec_reg_saturada <-
# Fórmula y datos
rec_viviendas |>
step_rm(has_role("Drop_Columns")) |>
step_mutate(across(all_numeric_predictors(), function(x) { ifelse(abs(scores(x,type = "z")) > 2.5 & !is.na(x), NA, x) })) |>
step_impute_bag(all_numeric_predictors()) |>
step_impute_mode(has_role("cuali")) |>
step_BoxCox(all_numeric_predictors()) |>
step_other(has_role("cuali"), threshold = .1, other = "Otros")  |>
# Normalizamos
step_dummy(all_nominal(), -all_outcomes())  |>
step_normalize(all_predictors()) |>
step_zv(all_predictors()) |>
step_corr(all_numeric_predictors(), threshold = 0.8)
rec_reg_saturada
prepped_data <-
rec_reg_saturada |>  # use the recipe object
prep() |>  # perform the recipe on training data
juice() # extract only the preprocessed dataframe
glimpse(prepped_data)
# Receta
rec_reg_elastic <-
# Fórmula y datos
rec_viviendas |>
step_rm(has_role("Drop_Columns")) |>
step_mutate(across(all_numeric_predictors(), function(x) { ifelse(abs(scores(x,type = "z")) > 2.2 & !is.na(x), NA, x) })) |>
step_impute_knn(all_numeric_predictors()) |>
step_impute_mode(has_role("cuali")) |>
step_BoxCox(all_numeric_predictors()) |>
step_sqrt(SalePrice) |>
step_other(has_role("cuali"), threshold = .2, other = "Otros")  |>
# Normalizamos
step_dummy(all_nominal(), -all_outcomes())  |>
step_normalize(all_predictors()) |>
step_zv(all_predictors()) |>
step_corr(all_numeric_predictors(), threshold = 0.8)
rec_reg_elastic
prepped_data <-
rec_reg_elastic |>  # use the recipe object
prep() |>  # perform the recipe on training data
juice() # extract only the preprocessed dataframe
glimpse(prepped_data)
# Modelo lineal
reg_lineal <- linear_reg() %>% set_mode("regression") %>% set_engine("lm")
# Flujo de Trabajo
reg_wflow_viviendas <-
workflow() %>%
add_model(reg_lineal) %>%
add_recipe(rec_reg_viviendas)
# Evaluación del modelo a los datos de validación
reg_fit_viviendas <-
reg_wflow_viviendas %>%
fit_resamples(resamples = validation_data)
# Métricas de error
reg_fit_viviendas |> collect_metrics()
reg_fit_viviendas <-  reg_wflow_viviendas %>% fit(viviendas_train)
tidy(reg_fit_viviendas)
check_model(reg_fit_viviendas %>% extract_fit_engine())
confint(reg_fit_viviendas %>% extract_fit_engine())
image = "https://www.investopedia.com/thmb/mgpezimLowCvuivu5aBE_dChWDI=/750x0/filters:no_upscale():max_bytes(150000):strip_icc():format(webp)/ConfidenceInterval-387c2dddb10c457e9d6041039b5b6e2c.png"
cat(paste0('<center><img src="', image,  '"></center>'))
ajuste <- reg_fit_viviendas %>% extract_fit_engine()
lm(ajuste$residuals ~ ajuste$fitted.values) %>% anova()
lm(ajuste$residuals ~ I(ajuste$fitted.values^2) + ajuste$fitted.values) %>% anova()
#Revisar Homecedasticidad
check_heteroscedasticity(ajuste)
ggplot(
tibble("obs" = 1:length(ajuste$residuals),
"res" = ajuste$residuals),
aes(x = obs, y = res)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title="Diagnosis de Regresión", subtitle="Modelo Univariante")+
theme_economist()
ols_test_normality(ajuste$residuals)
durbinWatsonTest(ajuste)
glance(reg_fit_viviendas)
# Predecimos en tst
fit_viviendas <- reg_wflow_viviendas %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_viviendas %>% collect_metrics()
# Errores en test
pred_test <-
fit_viviendas %>%
collect_predictions() %>%
mutate(error = SalePrice - .pred)
pred_test
# Gráficos en test
ggplot(data = pred_test,
mapping = aes(x = .pred,
y = SalePrice)) +
geom_point(color = "#006EA1",
alpha = 0.6,
size = 4) +
# Diagonal
geom_abline(intercept = 0, slope = 1,
color = "orange", size = 1.2) +
theme_economist() +
labs(title = "Resultados regresión lineal univariante",
subtitle =
"Valores deberían estar cercanos a la diagonal",
caption =
"Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
x = "Predicciones",
y = "Valores reales")
# Modelo lineal
reg_lineal <- linear_reg() %>% set_mode("regression") %>% set_engine("lm")
# Flujo de Trabajo
reg_wflow_saturada <-
workflow() %>%
add_model(reg_lineal) %>%
add_recipe(rec_reg_saturada)
# Evaluación del modelo a los datos de validación
reg_fit_saturada <-
reg_wflow_saturada %>%
fit_resamples(resamples = validation_data)
# Métricas de error
reg_fit_saturada |> collect_metrics()
reg_fit_saturada <-  reg_wflow_saturada %>% fit(viviendas_train)
tidy(reg_fit_saturada)
check_model(reg_fit_saturada %>% extract_fit_engine())
confint(reg_fit_saturada %>% extract_fit_engine())
ajuste <- reg_fit_saturada %>% extract_fit_engine()
lm(ajuste$residuals ~ ajuste$fitted.values) %>% anova()
lm(ajuste$residuals ~ I(ajuste$fitted.values^2) + ajuste$fitted.values) %>% anova()
#Revisar Homecedasticidad
check_heteroscedasticity(ajuste)
ggplot(
tibble("obs" = 1:length(ajuste$residuals),
"res" = ajuste$residuals),
aes(x = obs, y = res)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(title="Diagnosis de Regresión", subtitle="Modelo Univariante")+
theme_economist()
ols_test_normality(ajuste$residuals)
durbinWatsonTest(ajuste)
glance(reg_fit_saturada)
# Predecimos en tst
fit_saturada <- reg_wflow_saturada %>% last_fit(split = viviendas_split)
# Evaluamos en test
fit_saturada %>% collect_metrics()
# Errores en test
pred_test <-
fit_saturada %>%
collect_predictions() %>%
mutate(error = SalePrice - .pred)
pred_test
# Gráficos en test
ggplot(data = pred_test,
mapping = aes(x = .pred,
y = SalePrice)) +
geom_point(color = "#006EA1",
alpha = 0.6,
size = 4) +
# Diagonal
geom_abline(intercept = 0, slope = 1,
color = "orange", size = 1.2) +
theme_economist() +
labs(title = "Resultados regresión lineal saturada",
subtitle =
"Valores deberían estar cercanos a la diagonal",
caption =
"Autor: Roberto Bonilla| Datos: Ames Housing Dataset",
x = "Predicciones",
y = "Valores reales")
# Modelo lineal
elastic_net <-
linear_reg(mixture = tune("alpha"), penalty = tune("lambda")) %>%
set_mode("regression") %>% set_engine("glmnet")
grid_elastic_net <-
expand_grid("alpha" = seq(0, 1, l = 5),
"lambda" = grid_regular(penalty(range = c(-4, 1)), levels = 40) %>%
pull(penalty))
wflow_elastic <-
workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)
mod_elastic <- wflow_elastic %>%
tune_grid(resamples = cv_folds, grid = grid_elastic_net,
control = control_grid(verbose = TRUE))
# Muestra del mejor modelo en R cuadrada
mod_elastic %>% show_best("rsq")
# Muestra del mejor modelo en RMSE
mod_elastic %>% show_best("rmse")
# Selección del mejor modelo en R cuadrada
mod_elastic %>% select_best("rsq")
# Métricas de error
mod_elastic |> collect_metrics()
mod_elastic %>% autoplot()
elastic_net <-
linear_reg(mixture = 0.25, penalty =7.443803) %>%
set_mode("regression") %>% set_engine("glmnet")
wflow_elastic <-
workflow() %>% add_recipe(rec_reg_elastic) %>% add_model(elastic_net)
mod_elastic_fit <-
wflow_elastic %>% fit(train_data)
tidy(mod_elastic_fit)
coeficientes <- tidy(mod_elastic_fit)
coeficientes
coeficientes <- tidy(mod_elastic_fit)
coeficientes |> filter(estimate==0)
