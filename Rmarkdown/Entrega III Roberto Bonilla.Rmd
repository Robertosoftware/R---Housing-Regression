---
title: "Entrega III Individual"
description: |
  Entrega SEMMA: KNN + CART + RANDOM FOREST
author:
  - name: Roberto Bonilla Ibarra
    url: 
    affiliation: Universidad Complutense de Madrid
    affiliation_url: 
date: "`r Sys.Date()`"
output:
    distill::distill_article:
        highlight: kate
        colorlinks: true
        code_folding: false
        toc: true            
        toc_depth: 3     
---

```{r setup, include = FALSE}
# Ajuste comunes de los chunk
knitr::opts_chunk$set(fig.width = 8, fig.asp = 1, out.width = "100%",
                      message = FALSE, warning = FALSE,
                      echo = TRUE, res = 400)
```

# Objetivo {#objetivo}

El objetivo de esta entrega es poder predecir si los individuos recibirán un sueldo mayor o menor o igual a 50 mil dólares mensuales basado en sus características socioeconómicas, usando tres diferentes algoritmos de clasificación (knn, árboles y random forest).

## Paquetes necesarios

Necesitaremos los siguientes paquetes

* **Análisis exploratorio numérico**: paquete `{skimr}`
* **Depuración y preprocesamiento**: paquete `{tidyverse}`
* **Modelización**: paquete `{tidymodels}` para modelos
* **Detección de outliers**: paquete `{outliers}`


```{r paquetes}
# Borramos
rm(list = ls())

# Paquetes
library(skimr) # resumen numérico
library(tidymodels) # depuración datos
library(tidyverse) # modelos
library(outliers) # outliers
library(timeDate) # fechas
library(ggthemes) # tema para graficar
library(ranger) # Random Forest 
library(corrr) # Crear correlaciones
library(corrplot) # Crear matriz de correlación
library(parallel) # Librerías de Cómputo en Paralelo
library(doParallel) # Librerías de Cómputo en Paralelo
library("rpart.plot") # Librería para visualizar la lógica dentro del algoritmo de árboles.
library(vip) # Librería para visualizar la importancia de las variables dentro del algoritmo de árboles.
```

# Datos {#datos}

Vamos ir a un **ejemplo real**, haciendo uso de un **dataset de adultos con distintas variables socioeconómicas.**

```{r}
adultos <- read_csv(file = "./datos/adults.csv")
```

Los datos forman parte de un **censo poblacional relacionado a la percepción económica** elaborado por US Census Bureau con 32,561 registros.

📚 **Detalle de variables**: <http://www.cs.toronto.edu/~delve/data/adult/adultDetail.html1>



## Análisis exploratorio inicial (numérico)

Antes de tomar ninguna decisión con los datos lo primero que deberíamos hacer es **echar un vistazo numérico** a cómo se comportan las variables. Dado que vamos a clasificar, lo primero que deberíamos observar es como se distribuyen los niveles de nuestra variable objetivo.

### Variables

Lo primero es conocer las variables

```{r}
glimpse(adultos)
```

* `age`: Edad de los individuos
* `workclass`: Tipo de trabajo de cada individuo
* `fnlwgt`: Número de personas con la misma edad y raza.
* `education,education_num`: Nivel de educación máximo
* `marital_status`: Situación sentimental del individuo.
* `occupation`: A lo que se dedica el individuo
* `relationship`: Cuál es la relación que tiene con su familia.
* `race`: Raza del individuo
* `sex`: Sexo del individuo
* `capital_gain`: Capital percibido
* `capital_loss`: Capital perdido
* `hours_per_week`: Horas de trabajo semanales
* `native_country`: País de origen
* `over_50k`: Si el individuo percibe un ingreso mayor a 50 mil dólares anuales o menor o igual a este.


Además con la función `skim()` del paquete `{skimr}` podemos **extraer algunas estadísticas básicas** de nuestros datos.

```{r skim}
# Resumen numérico
adultos |> skim()
```

Veamos los tipos de dato de las variables:

```{r}
sapply(adultos, class)
```

Podemos ver que tenemos que cambiar el tipo de dato de la variable capital loss y capital gain, veamos su distribución,

**Capital Gain**
```{r}
adultos |> 
  count(capital_gain) |>
  mutate(porc = 100*n/sum(n))

```

**Capital Loss**

```{r}
adultos |> 
  count(capital_loss) |>
  mutate(porc = 100*n/sum(n))
```

Hemos visto que son variables continuas, por la tanto las convertiremos en numéricas:

```{r}
adultos$capital_gain <- as.numeric(str_replace_all(adultos$capital_gain,',',''))
adultos$capital_loss <- as.numeric(str_replace_all(adultos$capital_loss,',',''))
```


### Balance la variable objetivo

El objetivo será **predecir si un adulto está destinado a recibir un ingreso mayor a 50 mil dólares anuales o menor o igual a este**, por lo que la variable `over_50k` será nuestra variable objetivo. Primer paso: conocer cómo se **distribuyen los niveles de la objetivo** (es binaria)


```{r}
adultos |> 
  count(over_50k) |>
  mutate(porc = 100*n/sum(n))
```

Podemos ver que nuestra variable objetivo está desbalanceada hacia las personas con un ingreso menor o igual a 50 mil dólares.


# Análisis Exploratorio Visual

Antes de poder tomar decisiones en cómo transformar nuestros datos es importante poder hacer un análisis detallado visual de nuestros datos y conocer cómo interactúan con la variable objetivo:

Para esto usaremos el tema del economist del paquete ggplothemes

## Distribución visual variable objetivo

La Variable Objetivo se encuentre desbalanceda, teniendo un 75% de casos donde las personas ganan menos que 50 mil dólares al año.

```{r}
ggplot(adultos, aes(x = over_50k, fill = over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Variable objetivo",
       subtitle = "Distribución de ingresos",
       x = "Cantidad", y = "Frecuencia") +
  theme_economist()

```


## Distribución visual variables continuas

### Variable Edad

```{r}
ggplot(adultos, aes(x = age, fill = over_50k)) +
    geom_density(alpha = .3) +
    labs(title = "Variable Edad",
       subtitle = "Distribución de Edad con diferencia de Variable Objetivo",
       x = "Edad", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que las personas que ganan más de 50 mil dólares al año tienden a ser de mayor edad en comparación con los que hacen menos o igual a 50 mil dólares al año, también podemos ver que la mayoría están entre 30 y 55 años.

```{r}
ggplot(adultos, aes(x = over_50k, y = age, color=over_50k)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Variable Edad",
       subtitle = "Distribución de Edad por Tipo de Ingreso",
       x = "Over 50k", y = "Cantidad") +
  theme_economist()
```

### Variable Capital Gain

Ahora veamos como se comporta la variable de Capital Ganado

```{r}
ggplot(adultos, aes(x = capital_gain, fill = over_50k)) +
    geom_density(alpha = .6) + # Grado de transparencia para ver la gráfica traspuesta
  scale_y_continuous(limits = c(0, .0001)) + # Para poder visualizar mejor la gráfica
    labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado con diferencia de Variable Objetivo",
       x = "Capital Ganado", y = "Frecuencia") +
  theme_economist()
```


Podemos ver que las personas que perciben un ingreso mayor a 50K USD tienden a tener capital ganado en comparación con los que no perciben un ingreso mayor a 50k USD.

```{r}
ggplot(adultos |>  filter(capital_gain<20000), aes(x = capital_gain, fill = over_50k)) +
    geom_histogram(position = "identity", alpha = 0.4, bins=20) + # Posición identity para que se puedan trasponer los histogramas
     labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado con diferencia de Variable Objetivo",
       x = "Capital Gain", y = "Frecuencia") +
  theme_economist()
```


Podemos corroborar lo que mencionamos arriba, donde si bien la mayoría de las personas no ganan capital, cuando ellas documentan que perciben cierto capital tienen más probabilidad de ser del grupo de personas que perciben un ingreso mayor a 50 K USD.

```{r}
ggplot(adultos, aes(x = capital_gain, y = education, color=over_50k)) +
  geom_point() +
   labs(title = "Variable Capital Ganado",
       subtitle = "Distribución de Capital Ganado por Educación") +
  theme_economist()
```

Podemos ver que las personas que tienen un nivel académico bajo tienden a reportar un capital ganado bajo o nulo y a la vez este repercute en el ingreso anual que perciben en el año.

### Variable Capital Loss

Ahora analizaremos la variable de Capital Perdido

```{r}
ggplot(adultos, aes(x = over_50k, y = capital_loss, color=over_50k)) +
  geom_boxplot() +
  stat_summary(fun.y = "mean", geom = "point", shape = 23, size = 3, fill = "white") +
   labs(title = "Variable Capital Perdido",
       subtitle = "Distribución del Capital Perdido",
       x = "Over 50k", y = "Cantidad") +
  theme_economist()
```

Podemos ver que los percentiles 25, 50 y 75 se encuentran en 0, señalando que la mayor parte de las personas no tienen capital perdido, sin embargo al ver la distribución podemos determinar que en el caso que una persona tenga perdida de capital, es muy probable que si ella percibe un ingreso menor a 50 mil USD sea baja y visceversa, determinando esto también por donde se posiciona la media de cada grupo. (Las personas conun ingreso más alto son menos en los casos de perdida, sin embargo, su media es más alta) 

```{r}

ggplot(adultos, aes(x = capital_loss, fill = over_50k)) +
    geom_density(alpha = .6) +
  scale_y_continuous(limits = c(0, .002)) +
    labs(title = "Variable Capital Perdido",
       subtitle = "Distribución de Capital Perdidoe con diferencia por Tipo de Ingreso",
       x = "Capital Perdido", y = "Frecuencia") +
  theme_economist()
```

Esta gráfica confirma la observación marcada anteriormente con respecto al capital perdido, lo importante a resaltas es que al ser está gráfica la densidad de la distribución se aprecia de mejor manera.

### Variable Hours_per_week

```{r}

ggplot(adultos, aes(x =  hours_per_week, fill = over_50k)) +
    geom_density(position = "identity", alpha = 0.4, bins=20) +
    labs(title = "Variable Horas a la Semana",
       subtitle = "Distribución de Horas a la Semana con diferencia de Variable Objetivo",
       x = "Horas a la Semana", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que las personas con un ingreso 

### Variable fnlwgt


```{r}

ggplot(adultos, aes(x = fnlwgt, fill = over_50k)) +
    geom_density(alpha = .3) +
    labs(title = "Variable fnlwgt",
       subtitle = "Distribución de Fnlwgt con diferencia de Variable Objetivo",
       x = "Fnlwgt", y = "Frecuencia") +
  theme_economist()
```

Podemos ver que no existe ningún patrón entre las dos distribuciones.

## Distribución visual variables continuas

### Variable Clase de Trabajo

Analizaremos la variable **Clase de Trabajo** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = workclass, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Clases de Trabajo",
       subtitle = "Distribución de Clases de Trabajo",
       x = "Personas", y = "Clases de Trabajo") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

### Variable Estado Civil

Analizaremos la variable **Estado Civil** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = marital_status, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Estado Civil",
       subtitle = "Distribución del Estado Civil",
       x = "Personas", y = "Estado Civil") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver un comportamiento muy importante en la clase donde la gente se ha casado de manera civil, teniendo una gran cantidad y proporción mucho más favorable para los individuos que ganan más de 50 mil USD al año.

### Variable Ocupación

Analizaremos la variable *Ocupación** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = occupation, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Ocupación",
       subtitle = "Distribución de la variable Ocupación",
       x = "Personas", y = "Ocupación") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Como era de esperarse las clases que hacen una diferencia en su distribución son **Prof-Speciality** y **Exec-Managerial** por el tipo de ingreso que llevan estos dos grupos de personas.

### Variable Relación

Analizaremos la variable *Relación** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = relationship, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Relación",
       subtitle = "Distribución de la variable Relación",
       x = "Personas", y = "Relación") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Vemos aquí que los jedes de familia son en su mayoría los que perciben un ingreso mayor a 50 mil dólares anuales.

### Variable Raza

Analizaremos la variable *Raza** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = race, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Raza",
       subtitle = "Distribución de la variable Raza",
       x = "Personas", y = "Raza") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que los asiáticos y blancos son los más propensos a percibir un ingreso superior a 50 mil dólares anuales.

### Variable Sexo

Analizaremos la variable *Sexo** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = sex, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable Sexo",
       subtitle = "Distribución de la variable Sexo",
       x = "Personas", y = "Sexo") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(count)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que al agruparse la población, el sexo masculino tiene una mayor posibilidad de percibir un ingreso superior a 50 mil dólares anuales.


### Variable País

Analizaremos la variable *País** y su relación con la variable objetivo:

```{r}
  ggplot(adultos, aes(y = native_country, fill=over_50k)) +
  geom_bar(color = "white", alpha = 0.3) +
  labs(title = "Variable País",
       subtitle = "Distribución de la variable País en %",
       x = "Personas", y = "País") +
  geom_text(size = 2.5,stat='count', aes(label=after_stat(round(count / sum(count),2) * 100)), position=position_stack(vjust=0.5)) + 
  theme_economist()
```

Podemos ver que alrededor del 90% de las personas han nacido en Estados Unidos.

### Variable 

# Introducción Teórica

Explicaremos brevemente cada uno de los algoritmos que se usarán en esta práctica:


## KNN

Es uno de los modelos de Machine Learning más sencillos. 

Su lógica es asumir la similaridad entre un nuevo caso o dato y los otros puntos disponibles, colocando este nuevo punto en las categorías previamente definidas.

Algoritmo usado para regresión y clasificación.

Es un algoritmo de aprendizaje vago (lazy learner algorithm) ya que hace un almacenamiento de los datos de entrenamiento y espera hasta que reciba los datos de testing para poder empezar a clasificar, haciendo que tome menos tiempo en entrenamiento, pero más tiempo prediciendo.


El funcionamiento del algoritmo de KNN es el siguiente:

**Paso 1**: Seleccionamos el número K de vecinos.

**Paso 2**: Calculamos la distancia euclidea de los k vecinos.

**Paso 3**: Tomamos el K vecino más cercano

**Paso 4**: Entre los K vecinos contamos el número de datos que se encuentran en cada categoría 

**Paso 5**: Asignamos a la categoría que cuente con el mayor número de vecinos.

**Paso 6**: Contamos con el modelo listo

```{r echo = FALSE, results = 'asis'}
image = "https://datascientest.com/es/wp-content/uploads/sites/7/2020/11/Illu-2-KNN-1024x492.jpg"
cat(paste0('<center><img src="', image,  '"></center>')) 
```

Fuente: <https://www.javatpoint.com/k-nearest-neighbor-algorithm-for-machine-learning>

## Árboles de Decisión

Los arboles de decisión es un modelo de aprendizaje supervisado que sive para resolver problemas de clasificación y regresión, también conocido como `Classification And Regression Tree (CART) algorithm.`

Con base a la variable objetivo podemos definir los árboles de decisión en dos tipos:

1. Árboles de Decisión de Variables Categóricas
2. Árboles de Decisión de Variables Continuas

Un árbol de decisión es un clasificador en forma de árbol donde nodos internos representan variables del dataset, generando ramas que son reglas de decisión y cada hoja representa el resultado.

La fortaleza de un árbol de decisión es que son sumamente interpretables y representan la forma de pensar humana.

Para construir el modelo se puede sintetizar en los siguientes pasos:

**Paso 1**: Comenzar el árbol con el nodo raíz, llamado S, que contiene el conjunto de datos completo.

**Paso 2**: Encontrar el mejor atributo en el conjunto de datos usando la medida de selección de atributos (ASM).

**Paso 3**: Dividir el nod S en subconjuntos que contienen valores posibles para los mejores atributos.

**Paso 4**: Generar el nodo del árbol de decisión, que contiene el mejor atributo.

**Paso 5**: Hacer árboles de decisión nuevos de manera recursiva usando los subconjuntos del conjunto de datos creados en el paso 3. 
Continuar este proceso hasta que se alcance una etapa en la que no pueda clasificar más los nodos y llame al nodo final como un nodo hoja.


```{r echo = FALSE, results = 'asis'}
image = "https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png"
cat(paste0('<center><img src="', image,  '"></center>')) 
```

Fuente : <https://www.javatpoint.com/machine-learning-decision-tree-classification-algorithm> 

## Random Forest

Es un modelo de aprendizaje supervisado el cual nos sirve para regresión o clasificación, este se encuentra basado en el aprendizaje por ensamble, el cual es un proceso de combinar multiples clasificadores para un mejor resultado.

Como el nombre lo dice el modelo de `Random Forest` es un clasificador el cual contiene multiples árboles de decisión con una parte del dataset a entrenas para poder generar un promedio que mejore la precisión de la predición. Es importante que entre más número de árboles es mejor para el algoritmo en precisión y en evitar el sobreajuste.

El módelo de Random Forest funciona de la siguiente manera:

**Paso 1**: Seleccionamos k registos de nuestros datos

**Paso 2**: Construimos el árbol de decisión asociado al subsed de datos.

**Paso 3**: Seleccionamos el número N de árboles de decisión que se piensan construir.

**Paso 4**: Repetimos el paso 1 y el paso 2.

```{r echo = FALSE, results = 'asis'}
image = "https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm.png"
cat(paste0('<center><img src="', image,  '"></center>')) 
```

Fuente : <https://www.javatpoint.com/machine-learning-random-forest-algorithm> 

# Fase 1-2-3: muestreo-exploración-modificación

Ahora examinaremos los datos con el objetivo de poder tomar **decisiones que deberíamos adoptar**. Por ejemplo:

&nbsp;

* ¿Necesitamos **muestreo**? ¿De qué forma? ¿Podremos permitirnos crear un dataset de **validación**?

* ¿De qué **tipo** es cada variable? ¿Tenemos **problemas de codificación o rango**?

* ¿Cómo **afectan las predictoras** a los niveles de la variable objetivo?

* ¿Hay problemas de **dependencia** entre las variables?

* ¿Necesitamos **recategorizar** las variables? 

* ¿Tenemos **datos atípicos**?  ¿Tenemos **datos ausentes**? ¿Cómo imputarlos?

* ¿Necesitaremos hacer **One Hot Encoding** a nuestras variables categóricas?

&nbsp;

La **filosofía** será la siguiente: 

* modificaciones «estructurales» las hacemos fuera de la receta (modificando la base de datos)

* modificaciones más concretas para un algoritmo dentro de la receta (sin modificar la base de datos).

## Factores

Una de las primeras decisiones será dotar a las variables de su **tipología correcta**: debemos decidir si las variables de tipo texto son **variables cualitativas** (factores) o meros id's.

```{r}
adultos |>
  select(where(is.character)) |>
  glimpse()
```


Todas las variables de tipo texto representan **categorías de una cualitativa** así que las convertimos todas ellas a factor (modificación estructural --> fuera de la receta)

```{r}

adultos <- 
  adultos |>
  mutate(across(where(is.character),  ~str_replace_all(., ",", "")), across(where(is.character), as_factor))
adultos |> select(where(is.factor))
```

### Ordinales

Podemos ver que de todas nuestras variables la educación puede tener un orden en sus variables.

Primero veamos su distribución:

```{r}
adultos |>
  count(education) |> 
  mutate(porc = 100*n/sum(n))
```

Vemos que son 16 niveles de variable que reduciremos a 5 niveles.

Pero primero les quitaremos la coma al final


```{r}

adultos <- adultos |> 
 mutate(
    education_transformed = as.factor(case_when(
      education == "Doctorate"  ~ as.character("Grado 5"),
      education == "Prof-school" | education == "Masters"  ~ as.character("Grado 4"),
      education == "Some-college" | education == "Bachelors"  ~ as.character("Grado 3"),
      education == "Assoc-acdm" | education == "Assoc-voc"  ~ as.character("Grado 2"),
      TRUE ~ as.character("Grado 1")
    )))

knitr::kable(adultos |> select(education, education_transformed) |>  unique())
```


Tras ello convertiremos `education_transformed` a cualitativa pero ordinal.

```{r}
adultos <-
  adultos |>
  mutate(education_transformed = factor(education_transformed, levels = c("Grado 1", "Grado 2", "Grado 3", "Grado 4", "Grado 5"),
                       ordered = TRUE))
knitr::kable(adultos |> select(education_transformed) |> arrange(desc(education_transformed)) |> unique() )
```


## Variables cuali

Una vez convertidas en cualitativas analicemos cada una de ellas. La idea básica es la siguiente: ver que peso suponen cada nivel en las variables, y además, ver como **afectan los niveles a la variable objetivo**.

### Variable workclass

Ahora procederemos a ver cómo se distribuye la variable **clase de trabajo**:

```{r}
adultos |>
  count(workclass, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(workclass) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Self-emp-inc` y `Self-emp-not-inc` en un nuevo grupo llamado `Self-Emp` , así como `Local-Gov` y `State-Gov` en `Other-Gov`. También renombraremos el tipo **?** a `Desconocido`.

```{r}

adultos <- adultos |> 
 mutate(
    workclass_transformed = as.factor(case_when(
      workclass == "?"  ~ as.character("Desconocido"),
      workclass == "Local-gov" | workclass == "State-gov"  ~ as.character("Other-Gov"),
      workclass == "Self-emp-inc" | workclass == "Self-emp-not-inc"  ~ as.character("Self-Emp"),
      TRUE ~ as.character(workclass)
    )))

knitr::kable(adultos |> select(workclass, workclass_transformed) |>  unique())
```

### Variable Estado Civil

Ahora procederemos a ver cómo se distribuye la variable **Estado Civil**:

```{r}
adultos |>
  count(marital_status, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(marital_status) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Married-civ-spouse` y `Married-AF-spouse` en un nuevo grupo llamado `Married` , así como `Divorced`, `Separated`, `Widowed` y `Married-spouse-absent` en `Complicated`.

```{r}

adultos <- adultos |> 
 mutate(
    marital_status_transformed = as.factor(case_when(
      marital_status == "Married-civ-spouse" | marital_status == "Married-AF-spouse"  ~ as.character("Married"),
      marital_status == "Never-married"  ~ as.character("Never-married"),
      TRUE ~ as.character("Complicated")
    )))

knitr::kable(adultos |> select(marital_status, marital_status_transformed) |>  unique())
```



### Variable Ocupación

Ahora procederemos a ver cómo se distribuye la variable **Ocupación**:

```{r}
adultos |>
  count(occupation, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(occupation) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que hacen sentido como  `Prof-specialty` y `Exec-managerial` en un nuevo grupo llamado `High-Income` , así como `Protective-serv`, `Tech-support` y `Sales` en `Medium-Income` y el resto en `Low-Income`.

```{r}

adultos <- adultos |> 
 mutate(
    occupation_transformed = as.factor(case_when(
      occupation == "Prof-specialty" | occupation == "Exec-managerial"  ~ as.character("High-Income"),
      occupation == "Protective-serv" | occupation == "Tech-support" | occupation == "Sales"   ~ as.character("Medium-Income"),
      TRUE ~ as.character("Low-Income")
    )))

knitr::kable(adultos |> select(occupation, occupation_transformed) |>  unique())
```

### Variable Relación

Ahora procederemos a ver cómo se distribuye la variable **Relación**:

```{r}
adultos |>
  count(relationship, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(relationship) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```

Podemos apreciar que existen ciertas agrupaciones que pueden hacer sentido como  `Wife` y `Husband` en un nuevo grupo llamado `Boss` , así como el resto en un grupo llamado `Otros`. Esto lo haremos directamente en la receta para dejar al algorimto KNN con esta transformación y el resto de los algoritmos sin esta transformación.


### Variable Raza

Ahora procederemos a ver cómo se distribuye la variable **Raza**:

```{r}
adultos |>
  count(race, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(race) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso solo hace sentido agrupar `Amer-Indian-Eskimo` y `Other` en el grupo `Others`, las variables ya que son pocas y concisas, esto lo haremos directamente en la receta.


### Variable Sexo

Ahora procederemos a ver cómo se distribuye la variable **Sexo**:

```{r}
adultos |>
  count(sex, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(sex) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso no hace sentido agrupar ya que son dos categorías muy presentes.

### Variable País

Ahora procederemos a ver cómo se distribuye la variable **País**:

```{r}
adultos |>
  count(native_country, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumsum(porc))


adultos  |> group_by(native_country) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 

```
En este caso solo hace sentido dejar `United States` en un grupo, `Mexico` en otro grupo, `Philippines`,`Germany`,`Canada`,`India`,`England`,`South`,`Italy`,`Japan` y `Taiwan` en el grupo `High Income`, y el resto en el grupo `Others`.

```{r}

adultos <- adultos |> 
 mutate(
    country_transformed = as.factor(case_when(
      native_country %in% c("Philippines","Germany","Canada","India","England","South","Italy","Japan","Taiwan")  ~ "High-Income",
      native_country == "United-States"   ~ as.character(native_country),
      native_country == "Mexico"   ~ as.character(native_country),
      TRUE ~ as.character("Others")
    )))

knitr::kable(adultos |> select(native_country, country_transformed) |>  unique())
```


## Dependencia entre  cuali

Podemos ejecutar un **contraste de independencia** (prueba $\chi^2$ de independencia) para tener mayor certeza de si la predictora es dependiente o no de la variable objetivo (si fuera independiente, no tendría sentido mantenerla)

Podemos hacerlo con **todas las variables a la vez** enfrentándola a la objetivo, entendiendo que cada columna juega el rol de una lista si usamos las funciones del paquete `{purrr}`

```{r warning = FALSE}
chisq <-
  tibble("variable" = adultos |> select(where(is.factor)) |> names(),
         "p_value" = adultos |> select(where(is.factor)) |>
           map_dbl(.f = function(x) { chisq.test(adultos$over_50k, x)$p.value}))
chisq |> arrange(desc(p_value))
```


```{r warning = FALSE}
chisq |> filter(p_value > 0.05)
```

**No hay evidencia suficiente para decir que existe predictora independiente de la objetivo** (al 95% de confianza) según la prueba de independencia realizada


## Variables numéricas

Para las numéricas el proceso será ligeramente diferente, ya que ya no toman modalidades, aunque la mayoría de ellas como veremos podrían funcionar tanto de cuanti como de cuali (recategorizadas)

### Horas de Trabajo a la Semana

* `hours_per_week`: en realidad es una variable cualitativa más que cuantitativa, y a partir de 2 noches en festivo representa menos de 1% --> podríamos probar a **dejarla tal cual o recategorizarla en 4 categorías** (ninguna - 1 - 2 - más de 2)

```{r}
adultos |>
  count(hours_per_week, sort = TRUE) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc))
```

```{r}

adultos <- adultos |> 
 mutate(
    hours_per_week_transformed = as.factor(case_when(
      hours_per_week<40 ~ as.character("Menos de 40 Horas"),
      hours_per_week>40 ~ as.character("Más de 40 horas"),
      TRUE ~ as.character("40 Horas")
    )))

```

Ahora le daremos un orden a las nuevas categorías:

```{r}
adultos <-
  adultos |>
  mutate(hours_per_week_transformed = factor(hours_per_week_transformed, levels = c("Menos de 40 Horas", "40 Horas", "Más de 40 horas"),
                       ordered = TRUE))
knitr::kable(adultos |> select(hours_per_week_transformed) |> arrange(desc(hours_per_week_transformed)) |> unique() )
```

Y así se vería:

```{r}
adultos|> group_by(hours_per_week_transformed) |>  
  count(over_50k) |> 
  mutate(porc = 100*n/sum(n), cumul = cumsum(porc), nSum = sum(n)) |>  arrange(-nSum, -n) |> 
  select(-nSum) |> ungroup() 
```

## Colinealidad

Por último, nos falta comprobar los **problemas de  colinealidad** entre las predictoras numéricas. Podemos tratar las **numéricas por separado** (aunque tengamos muchas que en realidad hacen más de cuali que de cuanti)


```{r}
cor_matrix <- adultos |> select(where(is.numeric)) |> cor() |> round(2)

cor_matrix |>
  corrplot( method = 'ellipse', order = 'AOE', type = 'upper')
```


No parece existir una correlación elevada entre ninguna.


## Fase 3: modificación (fuera de la receta)

### Sampling

Con lo observado en la fase de exploración procederemos a hacer un **muestreo estratificado**  del 15% ya que tenemos muchas filas (al menos para hacer pruebas).


```{r}
# Muestreo del 25%
adultos_sample <-
  adultos |>
  group_by(over_50k) |> 
  slice_sample(prop = 0.15) |>
  ungroup()
```


## Fase 3: modificación (dentro de la receta)

### Partición

#### Test vs lo demás

Antes de nada deberemos realizar nuestras particiones, y primero dividiremos en **test y lo demás**, con `initial_split()`, teniendo 25% en test y 75% en todo lo demás

```{r}
# Partición 10% de test 
adultos_split <- initial_split(adultos_sample, strata = over_50k, prop = 0.75)
adultos_split
```

Fíjate que en `adultos_split` solo tenemos las instrucciones. Vamos a aplicarlas

```{r}
# Aplicamos partición
train_data <- training(adultos_split)
test_data <- testing(adultos_split)
```

Tras ello nunca está de más comprobar que efectivamente lo ha realizado de manera estratificada

```{r}
# Comprobamos estratos
train_data |> count(over_50k) |> mutate(porc = 100 * n / sum(n))
test_data |> count(over_50k) |> mutate(porc = 100 * n / sum(n))
```


#### Validación

Dado que no siempre disponemos de un volumen suficiente de datos, una **opción muy común y que arroja unos resultados bastante buenos** es la llamada **validación cruzada v-folds**: dividimos en $v$ trozos nuestro conjunto de entrenamiento, de forma que realizamos las siguientes iteraciones:

* Iteración 1: entrenamos el modelo con los conjuntos $\left\lbrace 2, 3, \ldots, v \right\rbrace$ y validamos con el primer conjunto.

* Iteración 2: entrenamos el modelo con los conjuntos $\left\lbrace 1, 3, \ldots, v \right\rbrace$ y validamos con el segundo conjunto.

...

* Iteración v: entrenamos el modelo con los conjuntos $\left\lbrace 1, 2, \ldots, v1 \right\rbrace$ y validamos con el conjunto $v$-ésimo.

Este método nos permite, no solo no tener que disponer de un alto volumen de datos sino que además **la validación ya no es sobre una sola muestra sino un promedio de varias** (eso sí, muestras relacionadas entre sí, no son independientes). Además con `strata` le diremos que dichas particiones las hagan estratificadas para conservar 0's-1's.


```{r}
# Fijamos la semilla
set.seed(100)

# Declaramos el número de particiones en las que procederemos a validar.
cv_folds <-
 vfold_cv(train_data, 
          v = 5, 
          strata = over_50k,
          repeats = 1) 
```

### Roles

Tras las particiones, el primer paso es **definir la receta**, indicándole el conjunto donde tenemos validación y train, y enfrentar `over_50k` con todas. Después lo que haremos será **asignar posibles roles** que nos puedan diferencias las acciones entre las variables


```{r}
# Receta
rec_adultos <-
  # Fórmula y datos
  recipe(data = train_data, over_50k ~ .)|>
  # Roles
  add_role(where(is.factor), new_role = "cuali") |> 
  add_role(where(is.numeric), new_role = "cuanti") |> 
  add_role(c(workclass, marital_status, occupation, native_country, hours_per_week, education, education_num), new_role = "Drop_Columns")
```


###Receta KNN

Ahora procederemos en crear la receta par el algoritmo KNN

```{r}
# Receta
KNN_rec_adultos <-
  rec_adultos  |> 
  # Modificación de variables actuales
  step_mutate(
    relationship =
                as.factor(case_when(
      relationship %in% c("Wife","Husband")  ~ "Boss",
      TRUE ~ "Others")),
    race =
                as.factor(case_when(
      race %in% c("Amer-Indian-Eskimo","Other")  ~ "Others",
      TRUE ~ as.character(race)))) |>
  # Eliminamos variables predefinidas.
  step_rm(has_role("Drop_Columns"))|> 
  # Detectar outliers
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "iqr")) > 1.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes (podríamos dejarlas como una categoría más)
  step_impute_knn(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Remover nulos
  step_naomit(everything(), skip = TRUE) |> 
  # Agrupamos en Minority los valores menos a 5%
  step_other(has_role("cuali"), threshold = .05, other = "Minority")  |> 
  # Escalamos los valores
  step_range(has_role("cuanti")) |> 
  # Normalizamos
  step_normalize(has_role("cuanti")) |> 
  # Dummyficamos
  step_dummy(all_nominal(), -all_outcomes())  |> 
  step_zv(all_predictors()) |> 
  step_corr(has_role("cuanti"), threshold = 0.7)  #|> 
```

Probaremos la receta para ver sus resultados:

```{r}
prepped_data <- 
  KNN_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(prepped_data)
```

### Receta CART

Ahora procederemos con la receta para crear el modelo de árboles de decisión.

```{r}
# Receta
cart_rec_adultos <-
  rec_adultos  |> 
  # Removeremos variables
  step_rm(has_role("Drop_Columns"))|> 
  # Detectaremos outliers con base al "z"test.
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "z")) >2.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes
  step_impute_median(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Removemos valores nulos
  step_naomit(everything(), skip = TRUE) |> 
  # Agrupamos en Minority los valores menos a 5%
  step_other(has_role("cuali"), threshold = .05, other = "Minority")  |> 
  # Eliminamos variables con varianza 0
  step_zv(all_predictors()) |> 
  # Eliminamos variables con correlación mayor al 70%
  step_corr(has_role("cuanti"), threshold = 0.7)  |> 
  # Sobremuestreo 66% - 33%
  themis::step_upsample(over_50k, over_ratio = 0.5) 
```

Probamos la receta:

```{r}
cart_prepped_data <- 
  cart_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(cart_prepped_data)
```

### Receta Random Forest

Ahora procederemos a crear la receta para la modelación con el algoritmo Random Forest:

```{r}
# Receta
rf_rec_adultos <-
  rec_adultos  |> 
  # Agru
  step_rm(has_role("Drop_Columns"))|> 
  # Detectar outliers
  step_mutate(across(where(is.numeric), function(x) { ifelse(abs(scores(x,type = "z")) >2.5 & !is.na(x), NA, x) })) |> 
  # Imputamos ausentes (podríamos dejarlas como una categoría más)
  step_impute_knn(has_role("cuanti")) |> 
  step_impute_mode(has_role("cuali")) |> 
  # Removemos nulos
  step_naomit(everything(), skip = TRUE) |>
  # Agrupamos con Tag Minority a los grupos de valores en las variables que representen menos del 4%
  step_other(has_role("cuali"), threshold = .04, other = "Minority")  |> 
  # Reagrupamos
  step_zv(all_predictors()) |> 
  step_corr(has_role("cuanti"), threshold = 0.7)  |> 
  # Sobremuestreo 50-50%
  themis::step_upsample(over_50k, over_ratio = 1) 
```


Probamos la receta para el algoritmo de Random Forest:

```{r}
rf_prepped_data <- 
  rf_rec_adultos |>  # use the recipe object
  prep() |>  # perform the recipe on training data
  juice() # extract only the preprocessed dataframe 

glimpse(rf_prepped_data)
```

# Fase 4 Modelling

## Metodología

Primero evaluaremos cada uno de los modelos por separado (KNN, CART, Random Forest) utilizando slice_sample seleccionaremos aleatoriamente 10 diferentes combinaciones de parámetros y con procesamiento en paralelo seleccionaremos el que tenga mejores resultados de cada uno de ellos. Al terminar la selección del mejor modelo, los compararemos entre ellos y seleccionaremos al mejor para hacer un Grid Search completo con validación cruzada y doble repetición para encontrar el mejor resultado posible.


## Cómputo en Paralelo

Vamos a **paralelizar en NUESTRO PROPIO ORDENADOR**: un ordenador suele tener **varios procesadores o cores** que pueden funcionar de manera «independiente» uno de otro. Vamos a detectar la cantidad de núcleos de los que podemos disponer con `detectCores()`.

```{r}
# Detectamos los cores que tenemos
detectCores()
```

A la hora de paralelizar es importante que lo hagamos con cuidado ya que puede que nuestro ordenador se quede colgado: mi consejo es que definas el número de cores a usar como los que tienes menos uno.

Con `makeCluster()` montamos los **clúster en cada nodo** y con `registerDoParallel()` registramos la paralelización (puedes ver los hilos abiertos con `showConnections()`).

```{r}
# Iniciamos la paralelización
clusters <- detectCores() - 1
make_cluster <- makeCluster(clusters)
registerDoParallel(make_cluster)
showConnections()
```

## Fase 4 KNN: modelo y flujo

### Flujo Modelo KNN

Empezamos a realizar el modelo KNN

```{r}
# Modelo
knn_model <-
  nearest_neighbor(mode = "classification", neighbors = tune("k"),
                   weight_func = tune("weight"), dist_power = tune("dist")) |>
  set_engine("kknn")

# Flujo de trabajo
knn_wflow_adultos <-
  workflow() |>
  add_recipe(KNN_rec_adultos) |>
  add_model(knn_model)
```

### Grid KNN

Ahora formaremos el grid para el modelo KNN

```{r}
grid_knn <-
  extract_parameter_set_dials(knn_wflow_adultos) |>
  # Actualizamos
  update(k = neighbors(range = c(90, 110)),
         weight = weight_func(values = c("inv","gaussian")),
         dist = dist_power(range = c(4, 6))) |>
  grid_regular(levels = 3) 
grid_knn <- grid_knn |> slice_sample(n=10) # 18 modelos (3 x 2 x 3)
grid_knn
```
### Aplicación del flujo de trabajo

```{r}
# Aplicamos el flujo
knn_fit_tune <- 
  knn_wflow_adultos |> 
  tune_grid(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas),
    grid = grid_knn,
    control = control_grid(verbose = TRUE, allow_par = TRUE, #<<
                                   pkgs = c("outliers")),
 ) 

# Best Model
knn_fit_tune |> show_best("roc_auc")

# Finalizamos flujo con el mejor modelo (según una métrica)
best_knn_model_auc <- knn_fit_tune |> select_best("roc_auc")

```

### Selección de Mejor Modelo

```{r}
final_knn_wf <- 
  knn_wflow_adultos |> 
  finalize_workflow(best_knn_model_auc)


final_knn_fit <- 
  final_knn_wf |>
  last_fit(adultos_split, 
           metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas)) 

# Recolectamos las métricas de error
final_knn_fit |> collect_metrics(summarize = TRUE)

```

### Matriz de Confusión

Ahora es importante saber cómo se visualiza la matriz de confusión

```{r}

knn_pred <- 
  final_knn_fit |>
  collect_predictions() 


knn_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```

### Curva ROC 

Hacemos la curva ROC para medir la calidad de la predicción

```{r}

knn_pred |> 
  group_by(id) |>
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot() +
  theme_economist_white()
```


### Probability Distribution Plot

Veamos la densidad de nuestra predicción.

```{r}

knn_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```

Podemos ver cómo nuestro modelo se comporta totalmente diferente al poder definir las personas con un ingreso mayor a 50 K las cuales se encuentran en su mayoría a partir del 77% de probabilidad de pertenencia.

## Fase 4 CART: modelo y flujo

### Flujo Modelo CART

Empezamos a realizar el modelo CART

```{r}
# Modelo
cart_model <- decision_tree <-
  decision_tree(mode = "classification",
                tree_depth = tune("depth"),
                min_n = tune("min_split"),
                cost_complexity = tune("cost")) |> 
  set_engine("rpart")

# Flujo de trabajo
cart_wflow_adultos <-
  workflow() |>
  add_recipe(cart_rec_adultos) |>
  add_model(cart_model)

# Parámetros a optimizar
param <- parameters(cart_model)
param$object

```

### Grid CART

```{r}

# Construimos el grid de parámetros
grid_tree <-
  parameters(cart_wflow_adultos) |>
  # Actualizamos
  update(depth = tree_depth(range = c(3, 8)),
         min_split = min_n(range = round(c(0.01, 0.1) * nrow(test_data),0)),
         cost = cost_complexity(range = c(-5, -1),
                                trans = log10_trans())) |>
  grid_regular(levels = 3) 
grid_tree_sample <- grid_tree |> slice_sample(n=10) # 10 modelos

grid_tree_sample
```


###  Aplicación del flujo de trabajo

```{r}
#Ejecución del flujo de trabajo
cart_fit_tune <- 
  cart_wflow_adultos |> 
  tune_grid(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas),
    grid = grid_tree_sample,
    control = control_grid(verbose = TRUE, allow_par = TRUE, #<<
                                   pkgs = c("outliers")),
 ) 

# Best Model
cart_fit_tune |> show_best("roc_auc")

# Finalizamos flujo con el mejor modelo (según una métrica)
best_cart_model_auc <- cart_fit_tune |> select_best("roc_auc")

```


### Selección del mejor modelo

```{r}
final_cart_wf <- 
  cart_wflow_adultos |> 
  finalize_workflow(best_cart_model_auc)


final_cart_fit <- 
  final_cart_wf |>
  last_fit(adultos_split, 
           metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas)) 

# Recolectamos las métricas de error
final_cart_fit |> collect_metrics(summarize = TRUE)

```

### Matriz de Confusión
```{r}

cart_pred <- 
  final_cart_fit |>
  collect_predictions()


cart_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```

Podemos ver que este modelo presenta mejores resultados que el KNN.

### Curva ROC
```{r}
# Matriz de confusión: etiqueta real vs etiqueta predicha
cart_pred |> 
  group_by(id) |> # id contains our folds
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot() +
  theme_economist_white()

```


### Probability Distribution Plot

```{r}
cart_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```


Podemos ver que el modelo de árboles de decisión es mejor distribuyendo la diferencia en proporciones al determinar si un individuo percibe o no 50 mil dólares anuales, generando múltiples máximos locales a través de su distribución.

### Visualizar el árbol

También podemos visualizar fácilmente los árboles generados con CART (Gini, del paquete `{rpart}`): primero con `extract_fit_engine()` extraemos las rutas del árbol, y después con `rpart.plot(roundint = FALSE, extra = 4)` le indicamos que no redondee valores a enteros y de los modelos de visualización elegimos `extra = 4` (prueba con varios para ver las diferencias). Para visualizar árboles `C5.0` ver documentación en <https://topepo.github.io/C5.0/reference/plot.C5.0.html>

```{r visualizar}
final_cart_fit |>
 extract_fit_engine() |>
  rpart.plot(roundint = FALSE, extra = 4)
```

### Importancia de las variables

Veamos las variables más importantes del árbol de decisión.

```{r}
final_cart_fit |> 
  pluck(".workflow", 1) |>   
  pull_workflow_fit() |> 
  vip(num_features = 10)
```

Vemos que la variable más importante es el papel que la persona tiene en su familia y su estado civil, esto puede ser muy lógico porque en el año que se recolectaron los datos existía cierta inclinación a ofrecer mejores beneficios a las personas con estabilidad familiar. 

## Fase 4 Random Forest: modelo y flujo

### Flujo Modelo Random Forest

Ahora empecemos con el flujo de trabajo del Random Forest:

```{r}
# Modelo
rf_spec <- 
  rand_forest(mtry = tune("p_mtry"), min_n = tune("p_min"), trees = tune("p_trees"))  |> 
  set_engine("ranger", importance = "impurity") |> 
  set_mode("classification")


# Flujo de trabajo
rf_wflow_adultos <-
 workflow() |>
 add_recipe(rf_rec_adultos) |> 
 add_model(rf_spec) 


# Parámetros a optimizar
param <- parameters(rf_wflow_adultos)
param$object

```

Podemos ver que este modelo cuenta con 3 parámetros a optimizar, los cuales son el número de arboles, el número de predictores y el número de nodos.

### Grid Random Forest


```{r}
# Construimos el grid de parámetros
grid_rf <-
  parameters(rf_wflow_adultos) |>
  # Actualizamos
  update(p_mtry = mtry(range = c(4, 9)),
         p_min = min_n(range = round(c(0.01, 0.1) * nrow(test_data),0)),
         p_trees = trees(range = c(5, 100))) |>
  grid_regular(levels = 4) 


grid_rf_sample <- grid_rf |> slice_sample(n=10) # 10 modelos

grid_rf_sample
```

### Aplicación del flujo de trabajo

```{r}
# Aplicamos el flujo
rf_fit_tune <- 
  rf_wflow_adultos |> 
  tune_grid(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas),
    grid = grid_rf_sample,
    control = control_grid(verbose = TRUE, allow_par = TRUE, #<<
                                   pkgs = c("outliers")),
 ) 

# Mejor modelo
rf_fit_tune |> show_best("roc_auc")

# Finalizamos flujo con el mejor modelo (según una métrica)
best_rf_model_auc <- rf_fit_tune |> select_best("roc_auc")

```
### Selección de Mejor Modelo

```{r}
final_rf_wf <- 
  rf_wflow_adultos |> 
  finalize_workflow(best_rf_model_auc)


final_rf_fit <- 
  final_rf_wf |>
  last_fit(adultos_split, 
           metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas)) 

# Recolectamos las métricas de error
final_rf_fit |> collect_metrics(summarize = TRUE)

```

### Matriz de Confusión

```{r}

rf_pred <- 
  final_rf_fit |>
  collect_predictions()


rf_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```

Podemos ver que existe una mejora importante entre el modelo de Random Forest y los dos anteriores, prediciendo de mejor manera los 

### Curva ROC


```{r}
# Matriz de confusión: etiqueta real vs etiqueta predicha
rf_pred |> 
  group_by(id) |> # id contains our folds
  roc_curve(over_50k, `.pred_<=50K`) |> 
  autoplot() +
  theme_economist_white()

```


### Probability Distribution Plot

```{r}
rf_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```

El modelo muestra una gran diferencia entre los dos cúmulos de probabilidades, cada uno inclinándose más hacia un lado y haciendo más certero al modelo.

### Importancia de las variables

```{r}
final_rf_fit |> 
  pluck(".workflow", 1) |>   
  pull_workflow_fit() |> 
  vip(num_features = 10)
```

Podemos ver que sigue siendo las dos variables más importantes el estado civil y la relación familiar para determinar el ingreso del individuo.

## Comparación de Modelos

Procederemos a comparar los 3 modelos que anteriormente hemos hecho.

```{r}
# Resultados Random Forest
rf_metrics <- 
  final_rf_fit |> 
  collect_metrics(summarise = TRUE) |>
  mutate(model = "Random Forest")

# Resultados Decision Tree
cart_metrics <- 
  final_cart_fit |> 
  collect_metrics(summarise = TRUE) |>
  mutate(model = "CART")

# Resultados KNN
knn_metrics <- 
  final_knn_fit |> 
  collect_metrics(summarise = TRUE) |>
  mutate(model = "KNN")

# Juntando resultados
model_compare <- bind_rows(rf_metrics,
                           cart_metrics,
                           knn_metrics,
                           ) 
# Modelando tabla para resultados
model_comp <- 
  model_compare |> 
  select(model, .metric, .estimate) |> 
  pivot_wider(names_from = .metric, values_from = c(.estimate)) 


# Graficando resultados 
model_comp |> 
  arrange(roc_auc) |> 
  mutate(model = fct_reorder(model, roc_auc)) |> # Ordenando Resultados
  ggplot(aes(model, roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(roc_auc, 2), y = roc_auc + 0.08),
     vjust = 1
  )
```

Siendo el modelo con mejor ROC_AUC el siguiente:

```{r}
model_comp |> slice_max(n=1, roc_auc)
```
 
### Comparación Curva ROC

Ahora veamos los resultados en una sola curva ROC.

```{r}
rf_pred_auc <- rf_pred |>  mutate(model= "rf")

cart_pred_auc <- cart_pred |>  mutate(model= "cart")

knn_pred_auc <- knn_pred |>  mutate(model= "knn")

roc_out <- bind_rows(rf_pred_auc, cart_pred_auc,knn_pred_auc)

roc_out %>%
  group_by(model) %>% # Agrupando valores
  roc_curve( truth = over_50k, `.pred_<=50K`) |> # Valores para graficar la curva ROC
  ggplot(
    aes(
      x = 1 - specificity, 
      y = sensitivity, 
      color = model
    )
  ) + # plot with 2 ROC curves for each model
  geom_line(size = 1.1) +
  geom_abline(slope = 1, intercept = 0, size = 0.4) +
  scale_color_manual(values = c("#48466D", "#3D84A8","#D24E01")) +
  coord_fixed() +
  theme_economist_white()
 
```

Podemos ver que el modelo que más cubre área en la grádica es el Random Forest y el que menos abarca es el modelo KNN
## Fase 4 Hyper Tunning 

### Validación con repetición

```{r}
# Declaramos el número de particiones en las que procederemos a validar.
cv_folds <-
 vfold_cv(train_data, 
          v = 5, 
          strata = over_50k,
          repeats = 2) 
```


### Aplicación de flujo de trabajo

Con base a los resultados podemos concluir que el modelo más adecuado es el **Random Forest** con los siguientes parámetros:

```{r}
# Best Model
rf_fit_tune |> show_best("roc_auc") |> slice_max(mean, n=1)

```

Con esto podemos optimizar el grid para encontrar el mejor modelo posible:

```{r}

# Construimos el grid de parámetros
grid_final_rf <-
  parameters(rf_wflow_adultos) |>
  # Actualizamos
  update(p_mtry = mtry(range = c(4, 6)),
         p_min = min_n(range = round(c(0.5, 0.15) * nrow(test_data),0)),
         p_trees = trees(range = c(20, 60))) |>
  grid_regular(levels = 3) 


grid_final_rf
```

Y aplicarlo al flujo de trabajo.

```{r}
# Aplicamos el flujo
rf_final_fit_tune <- 
  rf_wflow_adultos |> 
  tune_grid(
    resamples = cv_folds, 
    metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas),
    grid = grid_final_rf,
    control = control_grid(verbose = TRUE, allow_par = TRUE, #<<
                                   pkgs = c("outliers")),
 ) 

# Mejor Modelo
rf_final_fit_tune |> show_best("roc_auc")

# Finalizamos flujo con el mejor modelo (según una métrica)
best_rf_final_model_auc <- rf_final_fit_tune |> select_best("roc_auc")

```

Y finalizamos el cómputo en paralelo. (ya no será necesario)

```{r}
# finalizamos clusters
stopCluster(make_cluster)
registerDoSEQ()
```


### Selección de Mejor Modelo

Entrenaremos el modelo final con los parámetros del mejor modelo.

```{r}

final_rf_wf <- 
  rf_wflow_adultos |> 
  finalize_workflow(best_rf_final_model_auc)

final_rf_fit <- 
  final_rf_wf |>
  last_fit(adultos_split, 
           metrics = metric_set(recall, precision, accuracy,roc_auc, sens, f_meas)) 

final_rf_fit |> collect_metrics(summarize = TRUE)

```

### Matriz de Confusión

Visualizemos la matriz de confusión del modelo final

```{r}

rf_final_pred <- 
  final_rf_fit |>
  collect_predictions()


rf_final_pred |> 
  conf_mat(over_50k, .pred_class) |> 
  autoplot(type = "heatmap") 
```
Podemos ver que tiene un mejor performance que el modelo pasado, con una ligera mejora en el cuadrante verdadero positivo.


### Probability Distribution Plot

```{r}
rf_final_pred |> 
  ggplot() +
  geom_density(aes(x = `.pred_<=50K`, 
                   fill = over_50k), 
               alpha = 0.5) +
  theme_economist()
```
Podemos ver que el resultado es muy similiar al pasado Random Forest.

### Comparación entre modelos

Veamos una comparación entre los modelos construidos.

```{r}
rf_final_metrics <- 
  final_rf_fit |> 
  collect_metrics(summarise = TRUE) |>
  mutate(model = "Random Forest Final")

model_compare_vf <- bind_rows(model_compare,rf_final_metrics) 

model_comp <- 
  model_compare_vf |> 
  select(model, .metric, .estimate) |> 
  pivot_wider(names_from = .metric, values_from = c(.estimate)) 

 
model_comp |> 
  arrange(roc_auc) |> 
  mutate(model = fct_reorder(model, roc_auc)) |> # order results
  ggplot(aes(model, roc_auc, fill=model)) +
  geom_col() +
  coord_flip() +
  scale_fill_brewer(palette = "Blues") +
   geom_text(
     size = 3,
     aes(label = round(roc_auc, 2), y = roc_auc + 0.08),
     vjust = 1
  )
```
El modelo final tiene un AUC score un poco más alto que el pasado Random Forest.

### Curva ROC entre modelos

Ahora visualizemos la curva ROC de nuestros 4 modelos.

```{r}
rf_final_pred_auc <- rf_final_pred |>  mutate(model= "rf final")


roc_out_final <- bind_rows(rf_final_pred_auc, roc_out)

roc_out_final %>%
  group_by(model) %>% # group to get individual ROC curve for each model
  roc_curve( truth = over_50k, `.pred_<=50K`) |> # get values to plot an ROC curve
  ggplot(
    aes(
      x = 1 - specificity, 
      y = sensitivity, 
      color = model
    )
  ) + # plot with 2 ROC curves for each model
  geom_line(size = 1.1) +
  geom_abline(slope = 1, intercept = 0, size = 0.4) +
  scale_color_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#F0E442")) +
  coord_fixed() +
  theme_economist_white()
 
```
